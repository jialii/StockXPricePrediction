{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xuWdvCDwPXbR"
      },
      "source": [
        "#Backend Regression Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iOqovF1kPcAs",
        "outputId": "e3abee04-de7d-4414-d783-a386868f259a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting jupyter-dash\n",
            "  Downloading jupyter_dash-0.4.0-py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from jupyter-dash) (2.23.0)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.7/dist-packages (from jupyter-dash) (4.10.1)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.7/dist-packages (from jupyter-dash) (5.5.0)\n",
            "Collecting dash\n",
            "  Downloading dash-2.0.0-py3-none-any.whl (7.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.3 MB 31.5 MB/s \n",
            "\u001b[?25hCollecting ansi2html\n",
            "  Downloading ansi2html-1.6.0-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: flask in /usr/local/lib/python3.7/dist-packages (from jupyter-dash) (1.1.4)\n",
            "Requirement already satisfied: retrying in /usr/local/lib/python3.7/dist-packages (from jupyter-dash) (1.3.3)\n",
            "Collecting flask-compress\n",
            "  Downloading Flask_Compress-1.10.1-py3-none-any.whl (7.9 kB)\n",
            "Collecting dash-table==5.0.0\n",
            "  Downloading dash_table-5.0.0.tar.gz (3.4 kB)\n",
            "Collecting dash-html-components==2.0.0\n",
            "  Downloading dash_html_components-2.0.0.tar.gz (3.8 kB)\n",
            "Collecting dash-core-components==2.0.0\n",
            "  Downloading dash_core_components-2.0.0.tar.gz (3.4 kB)\n",
            "Collecting plotly>=5.0.0\n",
            "  Downloading plotly-5.5.0-py2.py3-none-any.whl (26.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 26.5 MB 16.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: click<8.0,>=5.1 in /usr/local/lib/python3.7/dist-packages (from flask->jupyter-dash) (7.1.2)\n",
            "Requirement already satisfied: Werkzeug<2.0,>=0.15 in /usr/local/lib/python3.7/dist-packages (from flask->jupyter-dash) (1.0.1)\n",
            "Requirement already satisfied: Jinja2<3.0,>=2.10.1 in /usr/local/lib/python3.7/dist-packages (from flask->jupyter-dash) (2.11.3)\n",
            "Requirement already satisfied: itsdangerous<2.0,>=0.24 in /usr/local/lib/python3.7/dist-packages (from flask->jupyter-dash) (1.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2<3.0,>=2.10.1->flask->jupyter-dash) (2.0.1)\n",
            "Collecting tenacity>=6.2.0\n",
            "  Downloading tenacity-8.0.1-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from plotly>=5.0.0->dash->jupyter-dash) (1.15.0)\n",
            "Collecting brotli\n",
            "  Downloading Brotli-1.0.9-cp37-cp37m-manylinux1_x86_64.whl (357 kB)\n",
            "\u001b[K     |████████████████████████████████| 357 kB 13.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tornado>=4.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel->jupyter-dash) (5.1.1)\n",
            "Requirement already satisfied: traitlets>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel->jupyter-dash) (5.1.1)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.7/dist-packages (from ipykernel->jupyter-dash) (5.3.5)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython->jupyter-dash) (2.6.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython->jupyter-dash) (4.4.2)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython->jupyter-dash) (1.0.18)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython->jupyter-dash) (57.4.0)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython->jupyter-dash) (4.8.0)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython->jupyter-dash) (0.8.1)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython->jupyter-dash) (0.7.5)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->jupyter-dash) (0.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from jupyter-client->ipykernel->jupyter-dash) (2.8.2)\n",
            "Requirement already satisfied: jupyter-core>=4.6.0 in /usr/local/lib/python3.7/dist-packages (from jupyter-client->ipykernel->jupyter-dash) (4.9.1)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.7/dist-packages (from jupyter-client->ipykernel->jupyter-dash) (22.3.0)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect->ipython->jupyter-dash) (0.7.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->jupyter-dash) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->jupyter-dash) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->jupyter-dash) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->jupyter-dash) (3.0.4)\n",
            "Building wheels for collected packages: dash-core-components, dash-html-components, dash-table\n",
            "  Building wheel for dash-core-components (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for dash-core-components: filename=dash_core_components-2.0.0-py3-none-any.whl size=3821 sha256=1479a9d619a8c4a3c8b65b49523a1ca79a77f252945288c5b5fbf951dd9ee014\n",
            "  Stored in directory: /root/.cache/pip/wheels/00/f9/c7/1a6437d794ed753ea9bc9079e761d4fc803a1f1f5d3697b9ec\n",
            "  Building wheel for dash-html-components (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for dash-html-components: filename=dash_html_components-2.0.0-py3-none-any.whl size=4089 sha256=91fff7968d1f301a00d0df51d0c8158be75e192bd99f35b51fb455bb3d564530\n",
            "  Stored in directory: /root/.cache/pip/wheels/ec/6b/81/05aceabd8b27f724e2c96784016287cc1bfbc349ebfda451de\n",
            "  Building wheel for dash-table (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for dash-table: filename=dash_table-5.0.0-py3-none-any.whl size=3911 sha256=25f820fb76002a00c05a15a39745eed4a17d95e79beab3fac4ea39043f6380c7\n",
            "  Stored in directory: /root/.cache/pip/wheels/85/5d/4e/7c276b57992951dbe770bf5caad6448d0539c510663aefd2e2\n",
            "Successfully built dash-core-components dash-html-components dash-table\n",
            "Installing collected packages: tenacity, brotli, plotly, flask-compress, dash-table, dash-html-components, dash-core-components, dash, ansi2html, jupyter-dash\n",
            "  Attempting uninstall: plotly\n",
            "    Found existing installation: plotly 4.4.1\n",
            "    Uninstalling plotly-4.4.1:\n",
            "      Successfully uninstalled plotly-4.4.1\n",
            "Successfully installed ansi2html-1.6.0 brotli-1.0.9 dash-2.0.0 dash-core-components-2.0.0 dash-html-components-2.0.0 dash-table-5.0.0 flask-compress-1.10.1 jupyter-dash-0.4.0 plotly-5.5.0 tenacity-8.0.1\n"
          ]
        }
      ],
      "source": [
        "!pip install jupyter-dash"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kUiUT_36Qy_j"
      },
      "source": [
        "Runtime may require restart after dash installation due to plotly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "icOKzBtoQgtx"
      },
      "outputs": [],
      "source": [
        "# Install the SQLAlchemy library if it is not installed\n",
        "!sudo apt-get install python3-dev libmysqlclient-dev > /dev/null\n",
        "!pip install mysqlclient > /dev/null\n",
        "!sudo pip3 install -U sql_magic > /dev/null\n",
        "!pip install psycopg2-binary > /dev/null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f1mtSeaDPTYk",
        "outputId": "0f079158-59e2-49c7-98b0-f3edb223788d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ]
        }
      ],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"Time series v2.ipynb\n",
        "\n",
        "Automatically generated by Colaboratory.\n",
        "\n",
        "Original file is located at\n",
        "    https://colab.research.google.com/drive/1LBp2nboSgIZ7rmIyocb_6suFdLjGjeiP\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# socail media sentiment, result, name of shoe'\n",
        "\n",
        "# Install the SQLAlchemy library if it is not installed\n",
        "# !sudo apt-get install python3-dev libmysqlclient-dev > /dev/null\n",
        "# !pip install mysqlclient > /dev/null\n",
        "# !sudo pip3 install -U sql_magic > /dev/null\n",
        "# !pip install psycopg2-binary > /dev/null\n",
        "\n",
        "# ! pip install pmdarima\n",
        "import datetime\n",
        "\n",
        "from sqlalchemy import create_engine\n",
        "import time\n",
        "import warnings\n",
        "import itertools\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import pandas as pd\n",
        "import statsmodels.api as sm\n",
        "import matplotlib\n",
        "import requests\n",
        "import json\n",
        "from bs4 import BeautifulSoup\n",
        "from datetime import datetime\n",
        "from datetime import date\n",
        "import re\n",
        "from pmdarima.arima import auto_arima\n",
        "\n",
        "# Commented out IPython magic to ensure Python compatibility.\n",
        "# %matplotlib inline\n",
        "# %config InlineBackend.figure_format = 'retina'\n",
        "import plotly.express as px\n",
        "\n",
        "def get_pushshift_data (data_type,     \n",
        "                   q,                 \n",
        "                   after,          \n",
        "                   size,               \n",
        "                   sort_type,\n",
        "                   sort):\n",
        "  url = f\"https://api.pushshift.io/reddit/search/{data_type}/\"\n",
        "  parameter = {\n",
        "      'data_type': data_type,\n",
        "      'q' : q,\n",
        "      'after' : after,\n",
        "      'size' : size,\n",
        "      'sort_type' : sort_type,\n",
        "      'sort' : sort,\n",
        "      'aggs' : 'subreddit' \n",
        "  }\n",
        "  request = requests.get(url,params = parameter)\n",
        "  return request.json()\n",
        "\n",
        "def dfpopulator(data,query,sku):\n",
        "  dic = {'Key':[],'sku':[],'Body':[],'Time(GMT)':[],'Subreddit':[],'Subreddit_id':[]}\n",
        "  for item in data['data']:\n",
        "    dic['Key'].append(query)\n",
        "    dic['sku'].append(sku)\n",
        "    dic['Body'].append(item['body'])\n",
        "    dic['Time(GMT)'].append(item['created_utc'])\n",
        "    dic['Subreddit'].append(item['subreddit'])\n",
        "    dic['Subreddit_id'].append(item['subreddit_id'])\n",
        "  \n",
        "  df = pd.DataFrame(dic)\n",
        "  return df\n",
        "  \n",
        "def redditSearch_Update(shoename,r_date,sku):\n",
        "    sneakers = [shoename]\n",
        "    data_type = 'comment'\n",
        "    duration = '500d'\n",
        "    size = 100;\n",
        "    sort_type = \"created_utc\"\n",
        "    sort = 'desc'\n",
        "    aggs='subreddit'\n",
        "    poped = False\n",
        "    column = ['Key','sku','Body','Time(GMT)','Subreddit','Subreddit_id']\n",
        "    shoedic = {}\n",
        "\n",
        "    shoedf = pd.DataFrame(columns = column)\n",
        "    for sneaker in sneakers:\n",
        "        utcdate = r_date\n",
        "        epoch_date = utcdate.timestamp()\n",
        "        data = get_pushshift_data(data_type,     \n",
        "                            sneaker,                 \n",
        "                            utcdate,          \n",
        "                            size,               \n",
        "                            sort_type,\n",
        "                            sort)\n",
        "        time.sleep(0.5)\n",
        "        data = dfpopulator(data,sneaker,sku)\n",
        "        shoedf = shoedf.append(data,ignore_index = True)\n",
        "    shoedf['Time(GMT)'] = pd.to_datetime(shoedf['Time(GMT)'],unit = 's')\n",
        "    shoedf = shoedf.applymap(lambda x: x.encode('ascii', 'ignore').decode('utf-8') if type(x) == str else x)\n",
        "    conn_string = 'mysql://{user}:{password}@{host}:{port}/{db}?charset=utf8'.format(\n",
        "    user='evergrand', \n",
        "    password='13Mterz/PFg=', \n",
        "    host = 'jsedocc7.scrc.nyu.edu', \n",
        "    port     = 3306, \n",
        "    encoding = 'utf-8',\n",
        "    db = 'Evergrand'\n",
        "    )\n",
        "\n",
        "    engine_reddit = create_engine(conn_string)\n",
        "    shoedf.to_sql(name='Reddit', # name the table \"inspections\"\n",
        "                   con=engine_reddit, # use the connection to MySQL created earlier\n",
        "                   if_exists='replace', # if the table is already there, replace it\n",
        "                   index=False, # do not write the index column in the database\n",
        "    )\n",
        "\n",
        "\n",
        "def twitterSearch_Update(q):\n",
        "  counter = 0\n",
        "  next_token = ''\n",
        "  url = f\"https://api.twitter.com/2/tweets/search/recent?query={q}&max_results=100&{next_token}tweet.fields=created_at\"\n",
        "  con = True;\n",
        "  column = ['created_at','id','text']\n",
        "  dflist = []\n",
        "  counter = 0\n",
        "  while (con):\n",
        "    # print(counter)\n",
        "    counter += 1\n",
        "    payload={\n",
        "    }\n",
        "    headers = {\n",
        "      'Authorization': 'Bearer AAAAAAAAAAAAAAAAAAAAAKYtWAEAAAAA2CtrQBw84WzcpTkYjnj%2BGKQ9bLg%3Dd5MPlt3uPPKp4dFVYbJEwPDFX5CuyB1d3CY4TyFZ3vr1fuVd5B',\n",
        "      'Cookie': 'guest_id=v1%3A163753755452701433; guest_id_ads=v1%3A163753755452701433; guest_id_marketing=v1%3A163753755452701433; personalization_id=\"v1_ZpL8SwtB/R5mdhJ27jO/HQ==\"'\n",
        "    }\n",
        "\n",
        "    response = requests.request(\"GET\", url, headers=headers, data=payload)\n",
        "    # print(response.text)\n",
        "\n",
        "    result = response.json()\n",
        "    if 'errors' in result:\n",
        "      badresult = pd.DataFrame(columns = column)\n",
        "      badresult['Name'] = ''\n",
        "      badresult['Name'] = q\n",
        "      badresult = badresult[['Name','created_at','id','text']]\n",
        "      to_append = [q,None,None,None]\n",
        "      a_series = pd.Series(to_append, index = badresult.columns)\n",
        "\n",
        "      badresult = badresult.append(a_series,ignore_index = True)\n",
        "      return badresult\n",
        "    newdata = pd.DataFrame(result['data'])\n",
        "    # print(newdata)\n",
        "    dflist.append(newdata)\n",
        "    # print(basedf)\n",
        "    if ('next_token' in result['meta']):\n",
        "      next_token = 'next_token='+result['meta']['next_token']+'&'\n",
        "      url = f\"https://api.twitter.com/2/tweets/search/recent?query={q}&max_results=100&{next_token}tweet.fields=created_at\"\n",
        "      time.sleep(1)\n",
        "\n",
        "    else:\n",
        "      con = False\n",
        "  print(q)\n",
        "  finalproduct = pd.concat(dflist)\n",
        "  finalproduct['Name'] = ''\n",
        "  finalproduct['Name'] = q\n",
        "  finalproduct = finalproduct[['Name','created_at','id','text']]\n",
        "  conn_string = 'mysql://{user}:{password}@{host}:{port}/{db}?charset=utf8'.format(\n",
        "    user='evergrand', \n",
        "    password='13Mterz/PFg=', \n",
        "    host = 'jsedocc7.scrc.nyu.edu', \n",
        "    port     = 3306, \n",
        "    encoding = 'utf-8',\n",
        "    db = 'Evergrand'\n",
        ")\n",
        "  engine_tweeter = create_engine(conn_string)\n",
        "#   %reload_ext sql_magic\n",
        "#   %config SQL.conn_name = 'engine_tweeter'\n",
        "  finalproduct = finalproduct.applymap(lambda x: x.encode('ascii', 'ignore').decode('utf-8') if type(x) == str else x)\n",
        "  print(finalproduct[1:5])\n",
        "  finalproduct[1:5].to_sql(name='Tweets', # name the table \"inspections\"\n",
        "                   con=engine_tweeter, # use the connection to MySQL created earlier\n",
        "                   if_exists='replace', # if the table is already there, replace it\n",
        "                   index=False, # do not write the index column in the database\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"#STEP 1: WEBSCRAPE RELEVANT INFORMATION\n",
        "\n",
        "Gets different attributes of a shoe based on the url input\n",
        "Attributes include name, release date, sku, and release price(MSRP)\n",
        "\"\"\"\n",
        "\n",
        "def getAttributeFromUrl(url):\n",
        "    headers = {\n",
        "    'sec-ch-ua': '\"Chromium\";v=\"94\", \"Google Chrome\";v=\"94\", \";Not A Brand\";v=\"99\"',\n",
        "    'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/94.0.4606.71 Safari/537.36',    'sec-ch-ua-platform': '\"Windows\"',\n",
        "    'sec-fetch-site': 'same-site',\n",
        "    'sec-fetch-dest': 'empty',\n",
        "    'accept-language': 'en-US,en;q=0.9',\n",
        "    }\n",
        "    r = requests.get(url, headers=headers)\n",
        "    soup = BeautifulSoup(r.text)\n",
        "    # test = soup.find_all('p',class_=\"chakra-text css-1h7lzbl\")\n",
        "    # print(test)\n",
        "    release_date = soup.find_all('p',class_=\"chakra-text css-imcdqs\")[3].text\n",
        "    release_price = soup.find_all('p',class_=\"chakra-text css-imcdqs\")[2].text\n",
        "\n",
        "    # if (release_date.startswith('20')):\n",
        "    #   datetime_obj = datetime.strptime(release_date,\"%Y/%m/%d\")\n",
        "    # else:\n",
        "    #   datetime_obj = datetime.strptime(release_date,\"%m/%d/%Y\")\n",
        "    name = soup.find_all('p',class_=\"chakra-text css-exht5z\")[0].text\n",
        "    sku = re.findall(r'sku\":\"(.*)\",\"c', r.text)[0]\n",
        "    placeholder = 1\n",
        "    valuelist = [name,placeholder,sku,release_price]\n",
        "    return valuelist\n",
        "\n",
        "\"\"\"Gets average daily sales data from StockX\"\"\"\n",
        "\n",
        "def getSalesFromInput(shoeurl,sku,today):\n",
        "    url = \"https://stockx.com/p/e\"\n",
        "\n",
        "    payload = json.dumps({\n",
        "      \"operationName\": \"FetchSalesGraph\",\n",
        "      \"variables\": {\n",
        "        \"isVariant\": False,\n",
        "        \"productId\": sku,\n",
        "        \"startDate\": \"all\",\n",
        "        \"endDate\": today,\n",
        "        \"intervals\": 100,\n",
        "        \"currencyCode\": \"USD\"\n",
        "      },\n",
        "      \"query\": \"query FetchSalesGraph($productId: String!, $currencyCode: CurrencyCode, $intervals: Int, $startDate: String, $endDate: String, $isVariant: Boolean! = false) {\\n  variant(id: $productId) @include(if: $isVariant) {\\n    salesChart(\\n      currencyCode: $currencyCode\\n      intervals: $intervals\\n      startDate: $startDate\\n      endDate: $endDate\\n    ) {\\n      ...SalesGraph\\n      __typename\\n    }\\n    __typename\\n  }\\n  product(id: $productId) @skip(if: $isVariant) {\\n    id\\n    title\\n    productCategory\\n    salesChart(\\n      currencyCode: $currencyCode\\n      intervals: $intervals\\n      startDate: $startDate\\n      endDate: $endDate\\n    ) {\\n      ...SalesGraph\\n      __typename\\n    }\\n    __typename\\n  }\\n}\\n\\nfragment SalesGraph on SalesChart {\\n  series {\\n    xValue\\n    yValue\\n    __typename\\n  }\\n  __typename\\n}\\n\"\n",
        "    })\n",
        "    headers = {\n",
        "      'authority': 'stockx.com',\n",
        "      'sec-ch-ua': '\" Not A;Brand\";v=\"99\", \"Chromium\";v=\"96\", \"Google Chrome\";v=\"96\"',\n",
        "      'apollographql-client-name': 'Iron',\n",
        "      'accept-language': 'en-US',\n",
        "      'sec-ch-ua-mobile': '?0',\n",
        "      'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36',\n",
        "      'content-type': 'application/json',\n",
        "      'accept': '/',\n",
        "      'apollographql-client-version': '2021.12.16.1',\n",
        "      'sec-ch-ua-platform': '\"Windows\"',\n",
        "      'origin': 'https://stockx.com',\n",
        "      'sec-fetch-site': 'same-origin',\n",
        "      'sec-fetch-mode': 'cors',\n",
        "      'sec-fetch-dest': 'empty',\n",
        "      'referer': shoeurl,\n",
        "      'cookie': '_ga=GA1.2.1853109428.1596137523; tracker_device=7f932b85-ff29-4b1f-a978-ce2e2afb45f1; _fbp=fb.1.1596137526024.1799719658; _px_f394gi7Fvmc43dfg_user_id=NWFhMzBiNjAtZDI5Yi0xMWVhLWJkMjUtZDVmMDk0ZDViODky; rskxRunCookie=0; rCookie=toixn79vpovnwmk8gutmmkd972mb6; QuantumMetricUserID=478d7a5bc264c78c0a6e7c5ff5d6cbde; mfaLogin=err; stockx_experiments_id=web-fc31ed57-2684-4bcc-a299-afd5de7e116d; _rdt_uuid=1621275189532.77d09608-2d35-461b-9e0f-d95973065349; _pk_id.421.1a3e=fa6ab7db943ac79b.1608758091.61.1622714543.1622714535.; __pdst=d6567c5c9b7843fc8ba6464d49be91d6; ajs_user_id=23303495-a855-11e7-afa5-128878ebb8b6; _ts_yjad=1629999961938; __ssid=6987566f28885ba3f26e3181d727ad9; _scid=1221c84a-c00d-4175-8c66-414b5227b647; _pxvid=d5c0f182-1723-11ec-aaf1-645079574d4f; _ga=GA1.2.1853109428.1596137523; _pin_unauth=dWlkPVpEZzJNek00WkRndE1XRTNNaTAwWTJJM0xUbGxPVEl0TUdJNE5EZ3hPREZsTXpNMA; __pxvid=3665192f-3795-11ec-a1ec-0242ac120003; _gcl_au=1.1.1441916943.1636602572; ajs_group_id=ab_3ds_messaging_eu_web.false%2Cab_aia_pricing_visibility_web.novariant%2Cab_chk_germany_returns_cta_web.true%2Cab_chk_order_status_reskin_web.false%2Cab_chk_place_order_verbage_web.false%2Cab_chk_recently_viewed_order_confirmation_web.false%2Cab_chk_remove_affirm_bid_entry_web.true%2Cab_chk_remove_fees_bid_entry_web.true%2Cab_citcon_psp_web.true%2Cab_desktop_home_hero_section_web.control%2Cab_home_contentstack_modules_web.variant%2Cab_low_inventory_badge_pdp_web.control%2Cab_pirate_payment_reorder_v2_web.variant_3%2Cab_pirate_recently_viewed_browse_web.false%2Cab_product_page_refactor_web.true%2Cab_recently_viewed_pdp_web.variant_1%2Cab_test_korean_language_web.true%2Cab_web_aa_1103.true; ajs_user_id=23303495-a855-11e7-afa5-128878ebb8b6; ajs_anonymous_id=4629f2c4-16f5-40ba-81c1-423ba2ef012b; token=; ajs_group_id=ab_3ds_messaging_eu_web.false%2Cab_aia_pricing_visibility_web.novariant%2Cab_chk_germany_returns_cta_web.true%2Cab_chk_order_status_reskin_web.false%2Cab_chk_place_order_verbage_web.false%2Cab_chk_remove_affirm_bid_entry_web.true%2Cab_chk_remove_fees_bid_entry_web.true%2Cab_citcon_psp_web.true%2Cab_desktop_home_hero_section_web.control%2Cab_home_contentstack_modules_web.variant%2Cab_home_dynamic_content_targeting_web.variant%2Cab_low_inventory_badge_pdp_web.control%2Cab_pirate_recently_viewed_browse_web.false%2Cab_product_page_refactor_web.true%2Cab_recently_viewed_pdp_web.variant_1%2Cab_test_korean_language_web.true%2Cab_web_aa_1103.true; ajs_anonymous_id=4629f2c4-16f5-40ba-81c1-423ba2ef012b; language_code=en; stockx_market_country=US; _gid=GA1.2.1580341715.1639858083; pxcts=34d1db51-603e-11ec-b2c9-379611497244; hide_my_vices=false; riskified_recover_updated_verbiage=true; ops_banner_id=blt055adcbc7c9ad752; IR_gbd=stockx.com; stockx_preferred_market_activity=sales; stockx_default_sneakers_size=All; stockx_default_streetwear_size=All; _clck=sj015u|1|exe|0; QuantumMetricSessionID=26abe8546fdd386fb58cbb0ec2466002; stockx_homepage=sneakers; stockx_session=%221710e3b6-416f-4848-bb6b-ffc319d1cc10%22; stockx_product_visits=5; IR_9060=1639873437892%7C0%7C1639873437892%7C%7C; IR_PI=59c8d844-d29b-11ea-a852-42010a246e2b%7C1639959837892; forterToken=ecbab609917d450281f829178b7e7def_1639873437228__UDF43_13ck; lastRskxRun=1639873440198; _px3=09501e9b6bf87059145143c8edfa674d1045e21fbf31ee08c7affe41e863d8bd:WGN7FyBy6rerAupD/A7AlTvSTGeHPH4KvI11cOn1VhFz6ygQY3HHamxbp6Uyt1bdyq21I8E6JMIHn42Jss+eTg==:1000:aIVG2m+sR6rb1di3NBqFjci4JZrtErPPhG28tvjv7xfexK+WZmAE2yLm9kRAMtsJM1lDl8Yy253gXx9Z9BclHw0q1UBA4fpzfI+j8F+Axd2LRXdM32fTKECUQd3qUJzJFYnN9q0SFg5LfbV0QltM+JgUaa8WKcG8AoUsZPG0CTQL91lSdXCkuXPovctSwW/FAf6H/rJwLukypQJOWfMiNWhcHJcTZkm7ZRQ6Ngn+PoM=; _clsk=wdnm0l|1639873780294|4|0|e.clarity.ms/collect; _gat=1; _uetsid=37a92700603e11ecbed3ebddd1d15633; _uetvid=c97149c0c3c111eba8f38109381a970d; _px_7125205957_cs=eyJpZCI6IjdiMDc1ZGQwLTYwNjAtMTFlYy05MzE3LTk5ZmE4Yzg5OGYyYSIsInN0b3JhZ2UiOnt9LCJleHBpcmF0aW9uIjoxNjM5ODc1NTg3OTk4fQ==; _dd_s=rum=0&expire=1639874688855'\n",
        "    }\n",
        "\n",
        "    response = requests.request(\"POST\", url, headers=headers, data=payload)\n",
        "\n",
        "    return response\n",
        "\n",
        "def exo_convert(exo):\n",
        "  nobs = len(exo.index)\n",
        "  k = len(exo.columns)\n",
        "  print(nobs,\" and k: \",k)\n",
        "\n",
        "  exog = np.empty([nobs, k])\n",
        "  for i in range(nobs):\n",
        "    k = 0;\n",
        "    for column in exo.columns:\n",
        "      exog[i,k] = exo[column].iloc[i]\n",
        "      k+=1\n",
        "\n",
        "    i+=1\n",
        "  return exog\n",
        "\n",
        "def getSentiment(text):\n",
        "    endpoint = \"https://api.us-east.natural-language-understanding.watson.cloud.ibm.com/instances/041cfe51-46b7-404a-baf4-88c9375ddb0c/v1/analyze\"\n",
        "\n",
        "    username = \"apikey\"\n",
        "    password = \"VqjPYYm8k1aHaYjkQpW1SimhfBmKLO6kjgSHAV4OZIo-\"\n",
        "\n",
        "    parameters = {\n",
        "        'features': 'emotion,sentiment',\n",
        "        'version' : '2021-08-01',\n",
        "        'text': text,\n",
        "        #'language' : 'en',\n",
        "       # 'encoding' : 'utf-8'\n",
        "    }\n",
        "\n",
        "    resp = requests.get(endpoint, params=parameters, auth=(username, password))\n",
        "    # if (resp.status_code == 400):\n",
        "    #     print(resp.text)\n",
        "    #     print(text)\n",
        "    return resp\n",
        "\"\"\"get sales data and transform them into more readable forms (changing column names to understandable values\n",
        "\n",
        "\"\"\"\n",
        "def main(shoeurl):\n",
        "  # assuming url has been received from user\n",
        "  # get sku from url\n",
        "  attributes = getAttributeFromUrl(shoeurl)\n",
        "  sku = attributes[2]\n",
        "  today = date.today().strftime(\"%Y-%m-%d\")\n",
        "  salesdic = getSalesFromInput(shoeurl,sku,today)\n",
        "  sales = (salesdic.json())['data']['product']['salesChart']['series']\n",
        "  dcol = ['__typename','xValue','yValue']\n",
        "\n",
        "  allsales = pd.DataFrame(sales)\n",
        "  print(attributes)\n",
        "  allsales['Date'] = pd.to_datetime(allsales['xValue']).dt.date\n",
        "  allsales['Date'] = pd.to_datetime(allsales['Date'])\n",
        "  allsales['Price'] = allsales['yValue']\n",
        "  allsales.drop(dcol,axis = 1, inplace = True)\n",
        "  allsales.dtypes\n",
        "  px.line(allsales,x = 'Date',y='Price')\n",
        "  # plt.show()\n",
        "\n",
        "  \"\"\"manually enter day zero sales data\n",
        "\n",
        "  day zero sales means the shoe sales in its retail price\n",
        "\n",
        "  the price of shoes tend to be much greater than the retail price, so here, I took the retail price, and entered its date as (earlest sales record date - 1 day) to represent the day 0 price\n",
        "  \"\"\"\n",
        "  datefirst = allsales['Date'][0]-pd.Timedelta(days = 1)\n",
        "  name = attributes[0]\n",
        "  twitterSearch_Update(name)\n",
        "  redditSearch_Update(name,datefirst,sku)\n",
        "\n",
        "  dayone = int(attributes[3][1:4])\n",
        "  start = {'Date':datefirst,'Price':dayone}\n",
        "  allsales = allsales.append(start,ignore_index = True,sort = True)\n",
        "\n",
        "  allsales.sort_values('Date',inplace=True)\n",
        "  print(allsales)\n",
        "\n",
        "  PCT = allsales\n",
        "  PCT['Daily_Change'] = allsales['Price'].pct_change()\n",
        "  # PCT.drop(['Price'],axis = 1,inplace = True)\n",
        "  PCT = PCT.fillna(0)\n",
        "  PCT\n",
        "\n",
        "  px.line(PCT,y='Daily_Change')\n",
        "\n",
        "  \"\"\"eliminate the duplicate dates that sometimes appear in stockx's website. \n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  PCT = PCT.drop_duplicates(subset = ['Date'],keep = 'last')\n",
        "  PCT\n",
        "  # PCT['Date']=PCT['Date'].asfreq('d')\n",
        "  # PCT = PCT.set_index('Date')\n",
        "  # PCT[PCT.index.duplicated()]\n",
        "\n",
        "  PCT = PCT.set_index('Date').asfreq('d')\n",
        "\n",
        "  PCT\n",
        "\n",
        "  PCT.drop('Price',axis = 1,inplace = True)\n",
        "\n",
        "  pd.set_option(\"display.max_rows\", 10)\n",
        "\n",
        "  PCT = PCT.fillna(0)\n",
        "  PCT.index.isnull().sum()\n",
        "  PCT\n",
        "\n",
        "  print(PCT.index.freq)\n",
        "\n",
        "  \"\"\"Note to self: remmeber to implement automatic find best\n",
        "\n",
        "  #Get Reddit posts and integrade into a dataframe of exogenous variables\n",
        "  \"\"\"\n",
        "\n",
        "\n",
        "\n",
        "  conn_string = 'mysql://{user}:{password}@{host}:{port}/{db}?charset=utf8'.format(\n",
        "      user='evergrand', \n",
        "      password='13Mterz/PFg=', \n",
        "      host = 'jsedocc7.scrc.nyu.edu', \n",
        "      port     = 3306, \n",
        "      encoding = 'utf-8',\n",
        "      db = 'Evergrand'\n",
        "  )\n",
        "  engine_visual = create_engine(conn_string)\n",
        "\n",
        "  # Commented out IPython magic to ensure Python compatibility.\n",
        "  # %reload_ext sql_magic\n",
        "  # %config SQL.conn_name = 'engine_visual'\n",
        "\n",
        "  # Commented out IPython magic to ensure Python compatibility.\n",
        "  # %%read_sql\n",
        "  # show tables\n",
        "\n",
        "  query = f'SELECT * FROM Reddit WHERE sku LIKE \"{sku}\"'\n",
        "  reddit_data = pd.read_sql(query,engine_visual)\n",
        "  reddit_data\n",
        "\n",
        "\n",
        "\n",
        "  query2 = f'SELECT * FROM Tweets WHERE Name LIKE \"{name}\"'\n",
        "  twt_data = pd.read_sql(query2,engine_visual)\n",
        "  twt_data\n",
        "  # query2 = f'SELECT * FROM Tweets WHERE Name LIKE \"{name}\"'\n",
        "  # twt_data = pd.read_sql(query2,engine_visual)\n",
        "  # twt_data\n",
        "\n",
        "\n",
        "\n",
        "  import requests\n",
        "  import json\n",
        "\n",
        "\n",
        "\n",
        "  twt_emotion = {\n",
        "          \"sadness\": [],\n",
        "          \"joy\": [],\n",
        "          \"fear\": [],\n",
        "          \"disgust\": [],\n",
        "          \"anger\": []\n",
        "        }\n",
        "  twtbodies = twt_data['text']\n",
        "  for body in twtbodies:\n",
        "    sentiment = getSentiment(body).json()\n",
        "    # print(sentiment)\n",
        "    # if sentiment['code'] == '400':\n",
        "    #   continue\n",
        "    for emotion in twt_emotion:\n",
        "      try:\n",
        "        twt_emotion[emotion].append(sentiment['emotion']['document']['emotion'][emotion])\n",
        "      except Exception as e:\n",
        "        continue\n",
        "\n",
        "  twtbodies\n",
        "\n",
        "  reddit_emotion = {\n",
        "          \"sadness\": [],\n",
        "          \"joy\": [],\n",
        "          \"fear\": [],\n",
        "          \"disgust\": [],\n",
        "          \"anger\": []\n",
        "        }\n",
        "  bodies = reddit_data['Body']\n",
        "  for body in bodies:\n",
        "    sentiment = getSentiment(body).json()\n",
        "\n",
        "    for emotion in reddit_emotion:\n",
        "      reddit_emotion[emotion].append(sentiment['emotion']['document']['emotion'][emotion])\n",
        "\n",
        "  bodies\n",
        "\n",
        "  twt_emotion\n",
        "\n",
        "  reddit_emotion\n",
        "\n",
        "  for emotion in reddit_emotion:\n",
        "    reddit_data[emotion] = reddit_emotion[emotion]\n",
        "\n",
        "  for emotion in twt_emotion:\n",
        "    twt_data[emotion] = twt_emotion[emotion]\n",
        "\n",
        "  twt_data\n",
        "\n",
        "  t_twt = twt_data\n",
        "\n",
        "  t_reddit = reddit_data\n",
        "\n",
        "#   reddit_data['Time(GMT)'] = pd.to_datetime(reddit_data['Time(GMT)'])\n",
        "  t_reddit['Date'] = pd.to_datetime(reddit_data['Time(GMT)'].dt.date)\n",
        "  t_reddit = t_reddit.set_index('Date')\n",
        "\n",
        "\n",
        "  t_twt['Date'] = pd.to_datetime(t_twt['created_at'])\n",
        "\n",
        "  t_twt = t_twt.set_index('Date')\n",
        "\n",
        "  t_twt = t_twt.resample('d').mean()\n",
        "  t_twt\n",
        "\n",
        "  t_reddit\n",
        "\n",
        "  t_reddit.dtypes\n",
        "\n",
        "  t_reddit\n",
        "\n",
        "  t_reddit.dtypes\n",
        "\n",
        "  drop_columns = ['sku','Body','Subreddit','Subreddit_id','Key','Time(GMT)']\n",
        "  t_reddit.drop(drop_columns,axis = 1, inplace = True)\n",
        "\n",
        "  time_index = PCT.index.to_series(name = 'exo')\n",
        "  time_index\n",
        "\n",
        "  time_index.asfreq('d')\n",
        "  time_index\n",
        "\n",
        "  t_reddit.sort_index(inplace = True)\n",
        "  exo = t_reddit.merge(time_index,how='right',on='Date')\n",
        "  # exo2 = pd.concat([time_index,t_twt], axis = 0,join = 'outer',ignore_index = False)\n",
        "  twt_index = pd.to_datetime(t_twt.index.to_series(name = 'twt').dt.date).asfreq('d')\n",
        "  twt_index\n",
        "\n",
        "  t_twt['New Date'] = twt_index\n",
        "  t_twt = t_twt.reset_index()\n",
        "  t_twt['Date'] = t_twt['New Date']\n",
        "  t_twt = t_twt.set_index('Date')\n",
        "  t_twt\n",
        "  exo2 = t_twt.merge(time_index,how='right',on='Date')\n",
        "  exo.drop(['exo'],axis = 1,inplace = True)\n",
        "  exo2.drop(['exo','New Date'],axis = 1,inplace = True)\n",
        "\n",
        "  # for column in range(len(exo.columns)):\n",
        "  #   for i in range(len(exo.index)):\n",
        "  #     if (pd.isna(exo.iloc[i,column]) and !pd.isna(exo2.iloc[i,column])):\n",
        "  #       exo.iat[i,column] == ((exo.iat[i,column]+exo2.iat[i,column])/2)\n",
        "  #     elif pd.isna(exo2.iat[i,column]):\n",
        "  #       exo.iat[i,column] = exo2.iat[i,column]\n",
        "  #     else:\n",
        "  #       exo.iat[i,column] = 0\n",
        "  # exo.iloc[1,3] = 10\n",
        "  exo = exo.combine_first(exo2)\n",
        "  media_sentiment = exo\n",
        "  pd.options.display.max_rows = 999\n",
        "  exo.fillna(method = 'bfill',inplace = True)\n",
        "  exo.fillna(method = 'ffill',inplace = True)\n",
        "  exo.replace(to_replace=0, method='ffill',inplace = True)\n",
        "  exo.replace(to_replace=0, method='bfill',inplace = True)\n",
        "\n",
        "  print(\"\\\\\\\\\\\\\\ntest exo\",exo)\n",
        "\n",
        "\n",
        "\n",
        "  \"\"\"Create off set in emotion values by 1 week\"\"\"\n",
        "\n",
        "  inx = exo.index\n",
        "  inx = inx.shift(7,freq = 'D')\n",
        "  exo.index = inx\n",
        "  exo.index.freq\n",
        "\n",
        "  \"\"\"Create train test split\n",
        "\n",
        "  Note that PCT train set starts on the same day that exo(which has been shifted by 7 days)\n",
        "  \"\"\"\n",
        "\n",
        "  end_date = PCT.index[len(PCT.index)-1]\n",
        "  end_date\n",
        "  pred_end = end_date + pd.Timedelta(days = 7)\n",
        "  pred_splittime = end_date + pd.Timedelta(days = 7)\n",
        "  pred_exo = exo.loc[(exo.index > end_date) & (exo.index <= pred_end)]\n",
        "  exo_full = exo.loc[(exo.index >= exo.index[0]) & (exo.index <= end_date)]\n",
        "  PCT_full = PCT_train = PCT.loc[(PCT.index >= exo.index[0]) & (PCT.index <= end_date)]\n",
        "  pred_exo = exo_convert(pred_exo)\n",
        "  exo_full = exo_convert(exo_full)\n",
        "\n",
        "  print(\"\\\\\\\\\\\\\\ntest exo_full \",exo_full)\n",
        "  print(\"\\\\\\\\\\\\\\ntest pred_exo \",pred_exo)\n",
        "  print(\"\\\\\\\\\\\\\\ntest PCT_full \",PCT_full)\n",
        "\n",
        "\n",
        "\n",
        "  split_time = end_date - pd.Timedelta(days = 7)\n",
        "  exo_train = exo.loc[(exo.index >= exo.index[0]) & (exo.index <= split_time)]\n",
        "  exo_test = exo.loc[(exo.index > split_time) & (exo.index <= end_date)]\n",
        "  print(\"\\\\\\\\\\\\\\ntest exo\",exo_test)\n",
        "  PCT_train = PCT.loc[(PCT.index >= exo.index[0]) & (PCT.index <= split_time)]\n",
        "  PCT_test = PCT.loc[(PCT.index > split_time) & (PCT.index <= end_date)]\n",
        "\n",
        "  PCT_print = PCT.loc[(PCT.index >= exo.index[0]) & (PCT.index <= end_date)]\n",
        "  PCT_test\n",
        "\n",
        "  pd.set_option(\"display.max_rows\", 114)\n",
        "\n",
        "  exo_train\n",
        "\n",
        "  exo_train = exo_convert(exo_train)\n",
        "  exo_test = exo_convert(exo_test)\n",
        "  from pylab import rcParams\n",
        "  rcParams['figure.figsize'] = 18, 8\n",
        "#   decomposition = sm.tsa.seasonal_decompose(PCT_train, model='additive')\n",
        "#   fig = decomposition.plot()\n",
        "  # plt.show()\n",
        "\n",
        "#   paramaters = auto_arima(PCT_train,max_p = 7,max_d = 2,trace = True)\n",
        "#   paramaters\n",
        "\n",
        "  d = q = range(0, 2)\n",
        "  p = range(0,7)\n",
        "  pdq = list(itertools.product(p, d, q))\n",
        "  seasonal_pdq = [(x[0], x[1], x[2], 53) for x in list(itertools.product(p, d, q))]\n",
        "\n",
        "\n",
        "\n",
        "  # for param in pdq:\n",
        "  #     for param_seasonal in seasonal_pdq:\n",
        "  #         try:\n",
        "  #             mod = sm.tsa.statespace.SARIMAX(PCT_train,\n",
        "  #                                             order=param,\n",
        "  #                                             seasonal_order=param_seasonal,\n",
        "  #                                             enforce_stationarity=False,\n",
        "  #                                             enforce_invertibility=False)\n",
        "  #             results = mod.fit()\n",
        "  #             current = results.alc\n",
        "  #             print('ARIMA{}x{}12 - AIC:{}'.format(param, param_seasonal, results.aic))\n",
        "  #         except:\n",
        "  #             continue\n",
        "\n",
        "  print(PCT_full.shape)\n",
        "\n",
        "  print(exo_full.shape)\n",
        "\n",
        "  mod = sm.tsa.statespace.SARIMAX(PCT_full,\n",
        "                                  exog = exo_full,\n",
        "                                  order=(0, 0, 0),\n",
        "                                  seasonal_order=(0, 0, 0, 365),\n",
        "                                  enforce_stationarity=False,\n",
        "                                  enforce_invertibility=False)\n",
        "  results = mod.fit()\n",
        "#   print(results.summary().tables[1])\n",
        "\n",
        "#   results.plot_diagnostics(figsize=(16, 8))\n",
        "  # plt.show()\n",
        "\n",
        "  \"\"\"#In sample testing\"\"\"\n",
        "  \n",
        "  end_date = PCT.index[len(PCT.index)-1]\n",
        "\n",
        "  end_date\n",
        "\n",
        "  pred_offset = pd.Timedelta(days = 7)\n",
        "  pred_start = end_date - pred_offset\n",
        "  pred_start\n",
        "  print(pred_start)\n",
        "\n",
        "  pred = results.get_prediction(start = pred_start,dynamic = False)\n",
        "\n",
        "  print(pred.predicted_mean)\n",
        "\n",
        "  pred = results.get_prediction(start = pred_start,dynamic = False)\n",
        "  pred_con = pred.conf_int()\n",
        "\n",
        "  ax = PCT_train.plot(label = 'observed')\n",
        "\n",
        "#   pred.predicted_mean.plot(ax = ax,label = 'One step ahead',alpha = 0.7,figsize = (14,7))\n",
        "\n",
        "#   ax.fill_between(pred_con.index,\n",
        "#                   pred_con.iloc[:, 0],\n",
        "#                   pred_con.iloc[:, 1], color='k', alpha=.2)\n",
        "\n",
        "\n",
        "\n",
        "  ax.set_xlabel ('Date')\n",
        "  ax.set_ylabel ('Percent change in price')\n",
        "  plt.legend()\n",
        "#   plt.show()\n",
        "\n",
        "  type(pred.predicted_mean)\n",
        "\n",
        "  price_forecasted = pred.predicted_mean\n",
        "  price_truth = PCT_train[pred_start:]\n",
        "  mse = ((price_forecasted - price_truth['Daily_Change']) ** 2).mean()\n",
        "  print(f'The Mean Squared Error of our forecasts is {mse}')\n",
        "\n",
        "\n",
        "  print('The Root Mean Squared Error of our forecasts is {}'.format(round(np.sqrt(mse), 10)))\n",
        "\n",
        "  \"\"\"#Out of sample testing\"\"\"\n",
        "\n",
        "#   end_date2 = PCT.index[-1]\n",
        "  end_date\n",
        "\n",
        "  pred_offset = pd.Timedelta(days = 7)\n",
        "  pred_start = end_date - pred_offset\n",
        "  pred_start\n",
        "  pred_endDate = end_date + pred_offset\n",
        "#   print(end_date,\"\\n\\n\\n\\n\\n\\n\\nhelllo \\n\\n\\n\\n\\n\\nand exotest: \",exo_test)\n",
        "  start_time = end_date + pd.Timedelta(days = 1)\n",
        "\n",
        "  print(start_time,\"\\n\\n\\n\\n\\n\\n\\nhelllo \\n\\n\\n\\n\\n\\nand pred_enddate and PCT FULL: \",pred_endDate,\"and PCT FULL: \",PCT_full)\n",
        "  exo3 = exo_test+exo_test\n",
        "#   print(end_date,\"\\n\\n\\n\\n\\n\\n\\nhelllo \\n\\n\\n\\n\\n\\nand exo3: \",exo3)\n",
        "\n",
        "  pred2 = results.predict(start = start_time,end = pred_endDate, exog=pred_exo)\n",
        "\n",
        "  # b = (exo_test.iloc[0]).reshape(1,5)\n",
        "  # pred2 = results.get_forecast(step = 1,exog = exo_test[1],dynamic = False)\n",
        "  pred_con = pred.conf_int()\n",
        "\n",
        "  ax2 = PCT_train.plot(label = 'observed')\n",
        "\n",
        "#   pred2.plot(ax = ax2,label = 'One step ahead',alpha = 0.7,figsize = (14,7))\n",
        "\n",
        "#   ax2.fill_between(pred_con.index,\n",
        "#                   pred_con.iloc[:, 0],\n",
        "#                   pred_con.iloc[:, 1], color='k', alpha=.2)\n",
        "\n",
        "\n",
        "\n",
        "  ax2.set_xlabel ('Date')\n",
        "  ax2.set_ylabel ('Percent change in price')\n",
        "#   plt.legend()\n",
        "  print(\"\\n\\n\\n\\n\\n\\n\\nhelllo  \\n\\n\\n\\n\\n\\nand the pred2: \",pred2)\n",
        "\n",
        "  return (pred2,media_sentiment,name)\n",
        "\n",
        "#   plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o28StqTyPcv1"
      },
      "source": [
        "#Frontend Dashboard"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yqsKB3-5aO5k"
      },
      "source": [
        "Due to compatiblity issues with Colab, Dash, and Ngrok, we are not able to display the fully interactive result in the notebook. Alternatively, we have put an example result in this notebook. We kindly ask you to run the .py file in your local enviroment for the full project including the chrome extension, homepage, and dynamic data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 672
        },
        "id": "HLEAGidMPzuE",
        "outputId": "284fea7c-47dc-4a68-b043-f65f684df934"
      },
      "outputs": [
        {
          "data": {
            "application/javascript": "(async (port, path, width, height, cache, element) => {\n    if (!google.colab.kernel.accessAllowed && !cache) {\n      return;\n    }\n    element.appendChild(document.createTextNode(''));\n    const url = await google.colab.kernel.proxyPort(port, {cache});\n    const iframe = document.createElement('iframe');\n    iframe.src = new URL(path, url).toString();\n    iframe.height = height;\n    iframe.width = width;\n    iframe.style.border = 0;\n    element.appendChild(iframe);\n  })(8050, \"/\", \"100%\", 650, false, window.element)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from dash import html, dcc\n",
        "from dash.dependencies import Input, Output, State\n",
        "import pandas as pd\n",
        "from jupyter_dash import JupyterDash\n",
        "from pandas import Timestamp\n",
        "\n",
        "app = JupyterDash(__name__)\n",
        "\n",
        "def generate_timeline_component(data, name):\n",
        "    '''\n",
        "    This function generates a timeline component for the prediction timeline\n",
        "    Takes in a dataframe and a name\n",
        "    '''\n",
        "    timelineFig = px.line(data, x=\"index\",\n",
        "                          y=\"Price\", template=\"plotly_dark\")\n",
        "    # timelineFig.update_xaxes(range=[0, 50])\n",
        "\n",
        "    timelineFig.update_layout(\n",
        "        xaxis_title=\"Price Prediction Timeline for the Next 14 Days Sales\", yaxis_title=\"Predicted Price\")\n",
        "\n",
        "    timelineComponent = html.Div(\n",
        "        className=\"chart-container vh-60\",\n",
        "        children=[\n",
        "            html.H3(children='Price Prediction of Selected Sneaker',),\n",
        "\n",
        "            html.Div(id=\"subheader\",\n",
        "                     children=f'''Price prediction for the sales average over the next 14 days for the {name}''',),\n",
        "\n",
        "            dcc.Graph(\n",
        "                id='price-graph',\n",
        "                figure=timelineFig,\n",
        "            ),\n",
        "            # dcc.RangeSlider(\n",
        "            #     id='timeline-slider',\n",
        "            #     min=0,\n",
        "            #     max=len(data)-1,\n",
        "            #     step=1,\n",
        "            #     # value=[data.index[0], data.index[-1]],\n",
        "            # )\n",
        "        ])\n",
        "    return timelineComponent\n",
        "\n",
        "\n",
        "def generate_social_volume_component(data):\n",
        "    '''\n",
        "    This function generates a social volume component for the prediction timeline\n",
        "    Takes in a dataframe\n",
        "    '''\n",
        "    socialFig = px.histogram(data, y=\"Subreddit\", x=\"Time(GMT)\",\n",
        "                             template=\"plotly_dark\", labels={\"Time(GMT)\": \"Time(GMT)\", \"Subreddit\": \"\"})\n",
        "\n",
        "    socialFig.update_yaxes(title=\"Social Media Volume\")\n",
        "\n",
        "    socialComponent = html.Div(\n",
        "        className=\"chart-container vh-40\",\n",
        "        children=[\n",
        "            html.Div(id=\"redditHeader\",\n",
        "                     children='''\n",
        "            Twitter & Reddit Volume Data''',),\n",
        "\n",
        "            dcc.Graph(\n",
        "                className=\"chart\",\n",
        "                id='reddit-graph',\n",
        "                figure=socialFig,\n",
        "            ),\n",
        "        ])\n",
        "\n",
        "    return socialComponent\n",
        "\n",
        "\n",
        "def generate_sentiment_component(data):\n",
        "    '''\n",
        "    This function generates a sentiment component for the prediction timeline\n",
        "    Takes in a dataframe\n",
        "    '''\n",
        "    sentimentChart = px.pie(data, values=\"Sentiment\",\n",
        "                            names=\"Sentiment Type\", template=\"plotly_dark\")\n",
        "\n",
        "    sentimentComponent = html.Div(\n",
        "        className=\"chart-container vh-40\",\n",
        "        children=[\n",
        "            html.Div(id=\"sentimentHeader\",\n",
        "                     children='''\n",
        "            Social Media Sentiment''',),\n",
        "\n",
        "            dcc.Graph(\n",
        "                className=\"chart\",\n",
        "                id='sentiment-graph',\n",
        "                figure=sentimentChart,\n",
        "            ),\n",
        "        ])\n",
        "\n",
        "    return sentimentComponent\n",
        "\n",
        "\n",
        "def generate_sentiment_callouts(most_positive, most_negative):\n",
        "\n",
        "    positiveCompoent = html.Div(\n",
        "        className=\"chart-container vh-10 w48\",\n",
        "        children=[html.Div(id=\"positiveHeader\", children='''Most Positive Social Media Content'''), dcc.Markdown(f'''*{most_positive[\"text\"]}* - {most_positive[\"user\"]}''')])\n",
        "\n",
        "    negativeCompoent = html.Div(\n",
        "        className=\"chart-container vh-10 w48\",\n",
        "        children=[html.Div(id=\"negativeHeader\", children='''Least Positive Social Media Content'''), dcc.Markdown(f'''*{most_negative[\"text\"]}* - {most_negative[\"user\"]}''')])\n",
        "\n",
        "    sentimentCalloutContainer = html.Div(\n",
        "        className=\"sentiment-row\",\n",
        "        children=[positiveCompoent, negativeCompoent])\n",
        "\n",
        "    return sentimentCalloutContainer\n",
        "\n",
        "# Core Components\n",
        "\n",
        "\n",
        "def generate_sidebar(social_volume_data, sentiment_data):\n",
        "\n",
        "    sidebarComponent = html.Div(\n",
        "        className=\"sidebar\",\n",
        "        children=[\n",
        "            generate_social_volume_component(social_volume_data),\n",
        "            generate_sentiment_component(sentiment_data),\n",
        "        ])\n",
        "    return sidebarComponent\n",
        "\n",
        "\n",
        "def generate_main_content(prediction_data, name, sentiment_callouts_data):\n",
        "\n",
        "    mainContentComponent = html.Div(\n",
        "        className=\"main-container two-thirds\",\n",
        "        children=[generate_timeline_component(prediction_data, name), generate_sentiment_callouts(sentiment_callouts_data[\"most_positive\"], sentiment_callouts_data[\"most_negative\"])])\n",
        "\n",
        "    return mainContentComponent\n",
        "\n",
        "\n",
        "def generate_content_component(prediction_data, name, sentiment_data, social_volume_data, sentiment_callout_data):\n",
        "\n",
        "    contentComponent = html.Div(\n",
        "        className=\"content\",\n",
        "        children=[generate_main_content(prediction_data, name, sentiment_callout_data), generate_sidebar(social_volume_data, sentiment_data)])\n",
        "\n",
        "    return contentComponent\n",
        "\n",
        "\n",
        "headerComponent = html.Div(\n",
        "    className=\"header\",\n",
        "    children=[\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        html.A(\n",
        "            html.Img(\n",
        "                src=app.get_asset_url(\"logo.jpg\"),\n",
        "                className=\"logo-img\",\n",
        "            ),\n",
        "        ),\n",
        "\n",
        "        html.H1(children='Sneaker Price Prediction Dashboard',),\n",
        "\n",
        "        html.A(\n",
        "            html.Img(\n",
        "                src=app.get_asset_url(\"homeIcon.png\"),\n",
        "                className=\"logo-img invert\",\n",
        "            ),\n",
        "        ),\n",
        "\n",
        "    ]\n",
        "\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "homeLayout = html.Div(\n",
        "    id=\"root\",\n",
        "    children=[headerComponent,\n",
        "              html.Div(\n",
        "\n",
        "\n",
        "                  [dcc.Input(\n",
        "                      id=\"input\",\n",
        "                      type=\"text\",\n",
        "                      placeholder=\"Enter StockX URL\",\n",
        "                  )] + [html.Div(id=\"out-all-types\"),\n",
        "                        dcc.Link('Submit', id=\"submitbtn\", href='/page-1', className=\"submit-button\")],\n",
        "\n",
        "                  className=\"input-container\", ),\n",
        "\n",
        "              ]\n",
        "\n",
        ")\n",
        "\n",
        "\n",
        "finalPredictionDF = pd.DataFrame({'index': {0: '12/22/2021',   1: '12/23/2021',   2: '12/24/2021',   3: '12/25/2021',   4: '12/26/2021',   5: '12/27/2021',   6: '12/28/2021',   7: '12/29/2021',   8: '12/30/2021',   9: '12/31/2021',   10: '1/1/2022',   11: '1/2/2022',   12: '1/3/2022',   13: '1/4/2022',   14: '1/5/2022'},  'predicted_mean': {0: -0.001757385,   1: -0.002289186,   2: -0.003470768,   3: 0.000793,   4: -0.000808121,   5: -0.000429289,   6: 0.001900562,   7: 0.004449323,   8: -0.001308924,   9: 0.003565347,   10: -4.09e-05,   11: 0.006289647,   12: 0.002974345,   13: 0.004632436,   14: 0.00484356},  'Price': {0: 329.4200631,   1: 328.6659594,   2: 327.5252362,   3: 327.7849636,   4: 327.5200738,   5: 327.3794729,   6: 328.0016779,   7: 329.4610632,   8: 329.0298237,   9: 330.2029294,   10: 330.1894334,   11: 332.2662086,   12: 333.2544829,   13: 334.798263,   14: 336.4198786}})\n",
        "\n",
        "# Social Volume Data\n",
        "\n",
        "resampledRedditData = pd.DataFrame({'Time(GMT)': {0: Timestamp('2020-07-13 00:00:00'),   1: Timestamp('2020-07-14 00:00:00'),   2: Timestamp('2020-07-15 00:00:00'),   3: Timestamp('2020-07-16 00:00:00'),   4: Timestamp('2020-07-17 00:00:00'),   5: Timestamp('2020-07-18 00:00:00'),   6: Timestamp('2020-07-19 00:00:00'),   7: Timestamp('2020-07-20 00:00:00'),   8: Timestamp('2020-07-21 00:00:00'),   9: Timestamp('2020-07-22 00:00:00'),   10: Timestamp('2020-07-23 00:00:00'),   11: Timestamp('2020-07-24 00:00:00'),   12: Timestamp('2020-07-25 00:00:00'),   13: Timestamp('2020-07-26 00:00:00'),   14: Timestamp('2020-07-27 00:00:00'),   15: Timestamp('2020-07-28 00:00:00'),   16: Timestamp('2020-07-29 00:00:00'),   17: Timestamp('2020-07-30 00:00:00'),   18: Timestamp('2020-07-31 00:00:00'),   19: Timestamp('2020-08-01 00:00:00'),   20: Timestamp('2020-08-02 00:00:00'),   21: Timestamp('2020-08-03 00:00:00'),   22: Timestamp('2020-08-04 00:00:00'),   23: Timestamp('2020-08-05 00:00:00'),   24: Timestamp('2020-08-06 00:00:00'),   25: Timestamp('2020-08-07 00:00:00'),   26: Timestamp('2020-08-08 00:00:00'),   27: Timestamp('2020-08-09 00:00:00'),   28: Timestamp('2020-08-10 00:00:00'),   29: Timestamp('2020-08-11 00:00:00'),   30: Timestamp('2020-08-12 00:00:00'),   31: Timestamp('2020-08-13 00:00:00'),   32: Timestamp('2020-08-14 00:00:00'),   33: Timestamp('2020-08-15 00:00:00'),   34: Timestamp('2020-08-16 00:00:00'),   35: Timestamp('2020-08-17 00:00:00'),   36: Timestamp('2020-08-18 00:00:00'),   37: Timestamp('2020-08-19 00:00:00'),   38: Timestamp('2020-08-20 00:00:00'),   39: Timestamp('2020-08-21 00:00:00'),   40: Timestamp('2020-08-22 00:00:00'),   41: Timestamp('2020-08-23 00:00:00'),   42: Timestamp('2020-08-24 00:00:00'),   43: Timestamp('2020-08-25 00:00:00'),   44: Timestamp('2020-08-26 00:00:00'),   45: Timestamp('2020-08-27 00:00:00'),   46: Timestamp('2020-08-28 00:00:00'),   47: Timestamp('2020-08-29 00:00:00'),   48: Timestamp('2020-08-30 00:00:00'),   49: Timestamp('2020-08-31 00:00:00'),   50: Timestamp('2020-09-01 00:00:00'),   51: Timestamp('2020-09-02 00:00:00'),   52: Timestamp('2020-09-03 00:00:00'),   53: Timestamp('2020-09-04 00:00:00'),   54: Timestamp('2020-09-05 00:00:00'),   55: Timestamp('2020-09-06 00:00:00'),   56: Timestamp('2020-09-07 00:00:00'),   57: Timestamp('2020-09-08 00:00:00'),   58: Timestamp('2020-09-09 00:00:00'),   59: Timestamp('2020-09-10 00:00:00'),   60: Timestamp('2020-09-11 00:00:00'),   61: Timestamp('2020-09-12 00:00:00'),   62: Timestamp('2020-09-13 00:00:00'),   63: Timestamp('2020-09-14 00:00:00'),   64: Timestamp('2020-09-15 00:00:00'),   65: Timestamp('2020-09-16 00:00:00'),   66: Timestamp('2020-09-17 00:00:00'),   67: Timestamp('2020-09-18 00:00:00'),   68: Timestamp('2020-09-19 00:00:00'),   69: Timestamp('2020-09-20 00:00:00'),   70: Timestamp('2020-09-21 00:00:00'),   71: Timestamp('2020-09-22 00:00:00'),   72: Timestamp('2020-09-23 00:00:00'),   73: Timestamp('2020-09-24 00:00:00'),   74: Timestamp('2020-09-25 00:00:00'),   75: Timestamp('2020-09-26 00:00:00'),   76: Timestamp('2020-09-27 00:00:00'),   77: Timestamp('2020-09-28 00:00:00'),   78: Timestamp('2020-09-29 00:00:00'),   79: Timestamp('2020-09-30 00:00:00'),   80: Timestamp('2020-10-01 00:00:00'),   81: Timestamp('2020-10-02 00:00:00'),   82: Timestamp('2020-10-03 00:00:00'),   83: Timestamp('2020-10-04 00:00:00'),   84: Timestamp('2020-10-05 00:00:00'),   85: Timestamp('2020-10-06 00:00:00'),   86: Timestamp('2020-10-07 00:00:00'),   87: Timestamp('2020-10-08 00:00:00'),   88: Timestamp('2020-10-09 00:00:00'),   89: Timestamp('2020-10-10 00:00:00'),   90: Timestamp('2020-10-11 00:00:00'),   91: Timestamp('2020-10-12 00:00:00'),   92: Timestamp('2020-10-13 00:00:00'),   93: Timestamp('2020-10-14 00:00:00'),   94: Timestamp('2020-10-15 00:00:00'),   95: Timestamp('2020-10-16 00:00:00'),   96: Timestamp('2020-10-17 00:00:00'),   97: Timestamp('2020-10-18 00:00:00'),   98: Timestamp('2020-10-19 00:00:00'),   99: Timestamp('2020-10-20 00:00:00'),   100: Timestamp('2020-10-21 00:00:00'),   101: Timestamp('2020-10-22 00:00:00'),   102: Timestamp('2020-10-23 00:00:00'),   103: Timestamp('2020-10-24 00:00:00'),   104: Timestamp('2020-10-25 00:00:00'),   105: Timestamp('2020-10-26 00:00:00'),   106: Timestamp('2020-10-27 00:00:00'),   107: Timestamp('2020-10-28 00:00:00'),   108: Timestamp('2020-10-29 00:00:00'),   109: Timestamp('2020-10-30 00:00:00'),   110: Timestamp('2020-10-31 00:00:00'),   111: Timestamp('2020-11-01 00:00:00'),   112: Timestamp('2020-11-02 00:00:00'),   113: Timestamp('2020-11-03 00:00:00'),   114: Timestamp('2020-11-04 00:00:00'),   115: Timestamp('2020-11-05 00:00:00'),   116: Timestamp('2020-11-06 00:00:00'),   117: Timestamp('2020-11-07 00:00:00'),   118: Timestamp('2020-11-08 00:00:00'),   119: Timestamp('2020-11-09 00:00:00'),   120: Timestamp('2020-11-10 00:00:00'),   121: Timestamp('2020-11-11 00:00:00'),   122: Timestamp('2020-11-12 00:00:00'),   123: Timestamp('2020-11-13 00:00:00'),   124: Timestamp('2020-11-14 00:00:00'),   125: Timestamp('2020-11-15 00:00:00'),   126: Timestamp('2020-11-16 00:00:00'),   127: Timestamp('2020-11-17 00:00:00'),   128: Timestamp('2020-11-18 00:00:00'),   129: Timestamp('2020-11-19 00:00:00'),   130: Timestamp('2020-11-20 00:00:00'),   131: Timestamp('2020-11-21 00:00:00'),   132: Timestamp('2020-11-22 00:00:00'),   133: Timestamp('2020-11-23 00:00:00'),   134: Timestamp('2020-11-24 00:00:00'),   135: Timestamp('2020-11-25 00:00:00'),   136: Timestamp('2020-11-26 00:00:00'),   137: Timestamp('2020-11-27 00:00:00'),   138: Timestamp('2020-11-28 00:00:00'),   139: Timestamp('2020-11-29 00:00:00'),   140: Timestamp('2020-11-30 00:00:00'),   141: Timestamp('2020-12-01 00:00:00'),   142: Timestamp('2020-12-02 00:00:00'),   143: Timestamp('2020-12-03 00:00:00'),   144: Timestamp('2020-12-04 00:00:00'),   145: Timestamp('2020-12-05 00:00:00'),   146: Timestamp('2020-12-06 00:00:00'),   147: Timestamp('2020-12-07 00:00:00'),   148: Timestamp('2020-12-08 00:00:00'),   149: Timestamp('2020-12-09 00:00:00'),   150: Timestamp('2020-12-10 00:00:00'),   151: Timestamp('2020-12-11 00:00:00'),   152: Timestamp('2020-12-12 00:00:00'),   153: Timestamp('2020-12-13 00:00:00'),   154: Timestamp('2020-12-14 00:00:00'),   155: Timestamp('2020-12-15 00:00:00'),   156: Timestamp('2020-12-16 00:00:00'),   157: Timestamp('2020-12-17 00:00:00'),   158: Timestamp('2020-12-18 00:00:00'),   159: Timestamp('2020-12-19 00:00:00'),   160: Timestamp('2020-12-20 00:00:00'),   161: Timestamp('2020-12-21 00:00:00'),   162: Timestamp('2020-12-22 00:00:00'),   163: Timestamp('2020-12-23 00:00:00'),   164: Timestamp('2020-12-24 00:00:00'),   165: Timestamp('2020-12-25 00:00:00'),   166: Timestamp('2020-12-26 00:00:00'),   167: Timestamp('2020-12-27 00:00:00'),   168: Timestamp('2020-12-28 00:00:00'),   169: Timestamp('2020-12-29 00:00:00'),   170: Timestamp('2020-12-30 00:00:00'),   171: Timestamp('2020-12-31 00:00:00'),   172: Timestamp('2021-01-01 00:00:00'),   173: Timestamp('2021-01-02 00:00:00'),   174: Timestamp('2021-01-03 00:00:00'),   175: Timestamp('2021-01-04 00:00:00'),   176: Timestamp('2021-01-05 00:00:00'),   177: Timestamp('2021-01-06 00:00:00'),   178: Timestamp('2021-01-07 00:00:00'),   179: Timestamp('2021-01-08 00:00:00'),   180: Timestamp('2021-01-09 00:00:00'),   181: Timestamp('2021-01-10 00:00:00'),   182: Timestamp('2021-01-11 00:00:00'),   183: Timestamp('2021-01-12 00:00:00'),   184: Timestamp('2021-01-13 00:00:00'),   185: Timestamp('2021-01-14 00:00:00'),   186: Timestamp('2021-01-15 00:00:00'),   187: Timestamp('2021-01-16 00:00:00'),   188: Timestamp('2021-01-17 00:00:00'),   189: Timestamp('2021-01-18 00:00:00'),   190: Timestamp('2021-01-19 00:00:00'),   191: Timestamp('2021-01-20 00:00:00'),   192: Timestamp('2021-01-21 00:00:00'),   193: Timestamp('2021-01-22 00:00:00'),   194: Timestamp('2021-01-23 00:00:00'),   195: Timestamp('2021-01-24 00:00:00'),   196: Timestamp('2021-01-25 00:00:00'),   197: Timestamp('2021-01-26 00:00:00'),   198: Timestamp('2021-01-27 00:00:00'),   199: Timestamp('2021-01-28 00:00:00'),   200: Timestamp('2021-01-29 00:00:00'),   201: Timestamp('2021-01-30 00:00:00'),   202: Timestamp('2021-01-31 00:00:00'),   203: Timestamp('2021-02-01 00:00:00'),   204: Timestamp('2021-02-02 00:00:00'),   205: Timestamp('2021-02-03 00:00:00'),   206: Timestamp('2021-02-04 00:00:00'),   207: Timestamp('2021-02-05 00:00:00'),   208: Timestamp('2021-02-06 00:00:00'),   209: Timestamp('2021-02-07 00:00:00'),   210: Timestamp('2021-02-08 00:00:00'),   211: Timestamp('2021-02-09 00:00:00'),   212: Timestamp('2021-02-10 00:00:00'),   213: Timestamp('2021-02-11 00:00:00'),   214: Timestamp('2021-02-12 00:00:00'),   215: Timestamp('2021-02-13 00:00:00'),   216: Timestamp('2021-02-14 00:00:00'),   217: Timestamp('2021-02-15 00:00:00'),   218: Timestamp('2021-02-16 00:00:00'),   219: Timestamp('2021-02-17 00:00:00'),   220: Timestamp('2021-02-18 00:00:00'),   221: Timestamp('2021-02-19 00:00:00'),   222: Timestamp('2021-02-20 00:00:00'),   223: Timestamp('2021-02-21 00:00:00'),   224: Timestamp('2021-02-22 00:00:00'),   225: Timestamp('2021-02-23 00:00:00'),   226: Timestamp('2021-02-24 00:00:00'),   227: Timestamp('2021-02-25 00:00:00'),   228: Timestamp('2021-02-26 00:00:00'),   229: Timestamp('2021-02-27 00:00:00'),   230: Timestamp('2021-02-28 00:00:00'),   231: Timestamp('2021-03-01 00:00:00'),   232: Timestamp('2021-03-02 00:00:00'),   233: Timestamp('2021-03-03 00:00:00'),   234: Timestamp('2021-03-04 00:00:00'),   235: Timestamp('2021-03-05 00:00:00'),   236: Timestamp('2021-03-06 00:00:00'),   237: Timestamp('2021-03-07 00:00:00'),   238: Timestamp('2021-03-08 00:00:00'),   239: Timestamp('2021-03-09 00:00:00'),   240: Timestamp('2021-03-10 00:00:00'),   241: Timestamp('2021-03-11 00:00:00'),   242: Timestamp('2021-03-12 00:00:00'),   243: Timestamp('2021-03-13 00:00:00'),   244: Timestamp('2021-03-14 00:00:00'),   245: Timestamp('2021-03-15 00:00:00'),   246: Timestamp('2021-03-16 00:00:00'),   247: Timestamp('2021-03-17 00:00:00'),   248: Timestamp('2021-03-18 00:00:00'),   249: Timestamp('2021-03-19 00:00:00'),   250: Timestamp('2021-03-20 00:00:00'),   251: Timestamp('2021-03-21 00:00:00'),   252: Timestamp('2021-03-22 00:00:00'),   253: Timestamp('2021-03-23 00:00:00'),   254: Timestamp('2021-03-24 00:00:00'),   255: Timestamp('2021-03-25 00:00:00'),   256: Timestamp('2021-03-26 00:00:00'),   257: Timestamp('2021-03-27 00:00:00'),   258: Timestamp('2021-03-28 00:00:00'),   259: Timestamp('2021-03-29 00:00:00'),   260: Timestamp('2021-03-30 00:00:00'),   261: Timestamp('2021-03-31 00:00:00'),   262: Timestamp('2021-04-01 00:00:00'),   263: Timestamp('2021-04-02 00:00:00'),   264: Timestamp('2021-04-03 00:00:00'),   265: Timestamp('2021-04-04 00:00:00'),   266: Timestamp('2021-04-05 00:00:00'),   267: Timestamp('2021-04-06 00:00:00'),   268: Timestamp('2021-04-07 00:00:00'),   269: Timestamp('2021-04-08 00:00:00'),   270: Timestamp('2021-04-09 00:00:00'),   271: Timestamp('2021-04-10 00:00:00'),   272: Timestamp('2021-04-11 00:00:00'),   273: Timestamp('2021-04-12 00:00:00'),   274: Timestamp('2021-04-13 00:00:00'),   275: Timestamp('2021-04-14 00:00:00'),   276: Timestamp('2021-04-15 00:00:00'),   277: Timestamp('2021-04-16 00:00:00'),   278: Timestamp('2021-04-17 00:00:00'),   279: Timestamp('2021-04-18 00:00:00'),   280: Timestamp('2021-04-19 00:00:00'),   281: Timestamp('2021-04-20 00:00:00'),   282: Timestamp('2021-04-21 00:00:00'),   283: Timestamp('2021-04-22 00:00:00'),   284: Timestamp('2021-04-23 00:00:00'),   285: Timestamp('2021-04-24 00:00:00'),   286: Timestamp('2021-04-25 00:00:00'),   287: Timestamp('2021-04-26 00:00:00'),   288: Timestamp('2021-04-27 00:00:00'),   289: Timestamp('2021-04-28 00:00:00'),   290: Timestamp('2021-04-29 00:00:00'),   291: Timestamp('2021-04-30 00:00:00'),   292: Timestamp('2021-05-01 00:00:00'),   293: Timestamp('2021-05-02 00:00:00'),   294: Timestamp('2021-05-03 00:00:00'),   295: Timestamp('2021-05-04 00:00:00'),   296: Timestamp('2021-05-05 00:00:00'),   297: Timestamp('2021-05-06 00:00:00'),   298: Timestamp('2021-05-07 00:00:00'),   299: Timestamp('2021-05-08 00:00:00'),   300: Timestamp('2021-05-09 00:00:00'),   301: Timestamp('2021-05-10 00:00:00'),   302: Timestamp('2021-05-11 00:00:00'),   303: Timestamp('2021-05-12 00:00:00'),   304: Timestamp('2021-05-13 00:00:00'),   305: Timestamp('2021-05-14 00:00:00'),   306: Timestamp('2021-05-15 00:00:00'),   307: Timestamp('2021-05-16 00:00:00'),   308: Timestamp('2021-05-17 00:00:00'),   309: Timestamp('2021-05-18 00:00:00'),   310: Timestamp('2021-05-19 00:00:00'),   311: Timestamp('2021-05-20 00:00:00'),   312: Timestamp('2021-05-21 00:00:00'),   313: Timestamp('2021-05-22 00:00:00'),   314: Timestamp('2021-05-23 00:00:00'),   315: Timestamp('2021-05-24 00:00:00'),   316: Timestamp('2021-05-25 00:00:00'),   317: Timestamp('2021-05-26 00:00:00'),   318: Timestamp('2021-05-27 00:00:00'),   319: Timestamp('2021-05-28 00:00:00'),   320: Timestamp('2021-05-29 00:00:00'),   321: Timestamp('2021-05-30 00:00:00'),   322: Timestamp('2021-05-31 00:00:00'),   323: Timestamp('2021-06-01 00:00:00'),   324: Timestamp('2021-06-02 00:00:00'),   325: Timestamp('2021-06-03 00:00:00'),   326: Timestamp('2021-06-04 00:00:00'),   327: Timestamp('2021-06-05 00:00:00'),   328: Timestamp('2021-06-06 00:00:00'),   329: Timestamp('2021-06-07 00:00:00'),   330: Timestamp('2021-06-08 00:00:00'),   331: Timestamp('2021-06-09 00:00:00'),   332: Timestamp('2021-06-10 00:00:00'),   333: Timestamp('2021-06-11 00:00:00'),   334: Timestamp('2021-06-12 00:00:00'),   335: Timestamp('2021-06-13 00:00:00'),   336: Timestamp('2021-06-14 00:00:00'),   337: Timestamp('2021-06-15 00:00:00'),   338: Timestamp('2021-06-16 00:00:00'),   339: Timestamp('2021-06-17 00:00:00'),   340: Timestamp('2021-06-18 00:00:00'),   341: Timestamp('2021-06-19 00:00:00'),   342: Timestamp('2021-06-20 00:00:00'),   343: Timestamp('2021-06-21 00:00:00'),   344: Timestamp('2021-06-22 00:00:00'),   345: Timestamp('2021-06-23 00:00:00'),   346: Timestamp('2021-06-24 00:00:00'),   347: Timestamp('2021-06-25 00:00:00'),   348: Timestamp('2021-06-26 00:00:00'),   349: Timestamp('2021-06-27 00:00:00'),   350: Timestamp('2021-06-28 00:00:00'),   351: Timestamp('2021-06-29 00:00:00'),   352: Timestamp('2021-06-30 00:00:00'),   353: Timestamp('2021-07-01 00:00:00'),   354: Timestamp('2021-07-02 00:00:00'),   355: Timestamp('2021-07-03 00:00:00'),   356: Timestamp('2021-07-04 00:00:00'),   357: Timestamp('2021-07-05 00:00:00'),   358: Timestamp('2021-07-06 00:00:00'),   359: Timestamp('2021-07-07 00:00:00'),   360: Timestamp('2021-07-08 00:00:00'),   361: Timestamp('2021-07-09 00:00:00'),   362: Timestamp('2021-07-10 00:00:00'),   363: Timestamp('2021-07-11 00:00:00'),   364: Timestamp('2021-07-12 00:00:00'),   365: Timestamp('2021-07-13 00:00:00'),   366: Timestamp('2021-07-14 00:00:00'),   367: Timestamp('2021-07-15 00:00:00'),   368: Timestamp('2021-07-16 00:00:00'),   369: Timestamp('2021-07-17 00:00:00'),   370: Timestamp('2021-07-18 00:00:00'),   371: Timestamp('2021-07-19 00:00:00'),   372: Timestamp('2021-07-20 00:00:00'),   373: Timestamp('2021-07-21 00:00:00'),   374: Timestamp('2021-07-22 00:00:00'),   375: Timestamp('2021-07-23 00:00:00'),   376: Timestamp('2021-07-24 00:00:00'),   377: Timestamp('2021-07-25 00:00:00'),   378: Timestamp('2021-07-26 00:00:00'),   379: Timestamp('2021-07-27 00:00:00'),   380: Timestamp('2021-07-28 00:00:00'),   381: Timestamp('2021-07-29 00:00:00'),   382: Timestamp('2021-07-30 00:00:00'),   383: Timestamp('2021-07-31 00:00:00'),   384: Timestamp('2021-08-01 00:00:00'),   385: Timestamp('2021-08-02 00:00:00'),   386: Timestamp('2021-08-03 00:00:00'),   387: Timestamp('2021-08-04 00:00:00'),   388: Timestamp('2021-08-05 00:00:00'),   389: Timestamp('2021-08-06 00:00:00'),   390: Timestamp('2021-08-07 00:00:00'),   391: Timestamp('2021-08-08 00:00:00'),   392: Timestamp('2021-08-09 00:00:00'),   393: Timestamp('2021-08-10 00:00:00'),   394: Timestamp('2021-08-11 00:00:00'),   395: Timestamp('2021-08-12 00:00:00'),   396: Timestamp('2021-08-13 00:00:00'),   397: Timestamp('2021-08-14 00:00:00'),   398: Timestamp('2021-08-15 00:00:00'),   399: Timestamp('2021-08-16 00:00:00'),   400: Timestamp('2021-08-17 00:00:00'),   401: Timestamp('2021-08-18 00:00:00'),   402: Timestamp('2021-08-19 00:00:00'),   403: Timestamp('2021-08-20 00:00:00'),   404: Timestamp('2021-08-21 00:00:00'),   405: Timestamp('2021-08-22 00:00:00'),   406: Timestamp('2021-08-23 00:00:00'),   407: Timestamp('2021-08-24 00:00:00'),   408: Timestamp('2021-08-25 00:00:00'),   409: Timestamp('2021-08-26 00:00:00'),   410: Timestamp('2021-08-27 00:00:00'),   411: Timestamp('2021-08-28 00:00:00'),   412: Timestamp('2021-08-29 00:00:00'),   413: Timestamp('2021-08-30 00:00:00'),   414: Timestamp('2021-08-31 00:00:00'),   415: Timestamp('2021-09-01 00:00:00'),   416: Timestamp('2021-09-02 00:00:00'),   417: Timestamp('2021-09-03 00:00:00'),   418: Timestamp('2021-09-04 00:00:00'),   419: Timestamp('2021-09-05 00:00:00'),   420: Timestamp('2021-09-06 00:00:00'),   421: Timestamp('2021-09-07 00:00:00'),   422: Timestamp('2021-09-08 00:00:00'),   423: Timestamp('2021-09-09 00:00:00'),   424: Timestamp('2021-09-10 00:00:00'),   425: Timestamp('2021-09-11 00:00:00'),   426: Timestamp('2021-09-12 00:00:00'),   427: Timestamp('2021-09-13 00:00:00'),   428: Timestamp('2021-09-14 00:00:00'),   429: Timestamp('2021-09-15 00:00:00'),   430: Timestamp('2021-09-16 00:00:00'),   431: Timestamp('2021-09-17 00:00:00'),   432: Timestamp('2021-09-18 00:00:00'),   433: Timestamp('2021-09-19 00:00:00'),   434: Timestamp('2021-09-20 00:00:00'),   435: Timestamp('2021-09-21 00:00:00'),   436: Timestamp('2021-09-22 00:00:00'),   437: Timestamp('2021-09-23 00:00:00'),   438: Timestamp('2021-09-24 00:00:00'),   439: Timestamp('2021-09-25 00:00:00'),   440: Timestamp('2021-09-26 00:00:00'),   441: Timestamp('2021-09-27 00:00:00'),   442: Timestamp('2021-09-28 00:00:00'),   443: Timestamp('2021-09-29 00:00:00'),   444: Timestamp('2021-09-30 00:00:00'),   445: Timestamp('2021-10-01 00:00:00'),   446: Timestamp('2021-10-02 00:00:00'),   447: Timestamp('2021-10-03 00:00:00'),   448: Timestamp('2021-10-04 00:00:00'),   449: Timestamp('2021-10-05 00:00:00'),   450: Timestamp('2021-10-06 00:00:00'),   451: Timestamp('2021-10-07 00:00:00'),   452: Timestamp('2021-10-08 00:00:00'),   453: Timestamp('2021-10-09 00:00:00'),   454: Timestamp('2021-10-10 00:00:00'),   455: Timestamp('2021-10-11 00:00:00'),   456: Timestamp('2021-10-12 00:00:00'),   457: Timestamp('2021-10-13 00:00:00'),   458: Timestamp('2021-10-14 00:00:00'),   459: Timestamp('2021-10-15 00:00:00'),   460: Timestamp('2021-10-16 00:00:00'),   461: Timestamp('2021-10-17 00:00:00'),   462: Timestamp('2021-10-18 00:00:00'),   463: Timestamp('2021-10-19 00:00:00'),   464: Timestamp('2021-10-20 00:00:00'),   465: Timestamp('2021-10-21 00:00:00'),   466: Timestamp('2021-10-22 00:00:00'),   467: Timestamp('2021-10-23 00:00:00'),   468: Timestamp('2021-10-24 00:00:00'),   469: Timestamp('2021-10-25 00:00:00'),   470: Timestamp('2021-10-26 00:00:00'),   471: Timestamp('2021-10-27 00:00:00'),   472: Timestamp('2021-10-28 00:00:00'),   473: Timestamp('2021-10-29 00:00:00'),   474: Timestamp('2021-10-30 00:00:00'),   475: Timestamp('2021-10-31 00:00:00'),   476: Timestamp('2021-11-01 00:00:00'),   477: Timestamp('2021-11-02 00:00:00'),   478: Timestamp('2021-11-03 00:00:00'),   479: Timestamp('2021-11-04 00:00:00'),   480: Timestamp('2021-11-05 00:00:00'),   481: Timestamp('2021-11-06 00:00:00'),   482: Timestamp('2021-11-07 00:00:00'),   483: Timestamp('2021-11-08 00:00:00'),   484: Timestamp('2021-11-09 00:00:00'),   485: Timestamp('2021-11-10 00:00:00'),   486: Timestamp('2021-11-11 00:00:00'),   487: Timestamp('2021-11-12 00:00:00'),   488: Timestamp('2021-11-13 00:00:00'),   489: Timestamp('2021-11-14 00:00:00'),   490: Timestamp('2021-11-15 00:00:00'),   491: Timestamp('2021-11-16 00:00:00'),   492: Timestamp('2021-11-17 00:00:00'),   493: Timestamp('2021-11-18 00:00:00'),   494: Timestamp('2021-11-19 00:00:00'),   495: Timestamp('2021-11-20 00:00:00')},  'Subreddit': {0: 1,   1: 1,   2: 1,   3: 0,   4: 0,   5: 0,   6: 0,   7: 0,   8: 0,   9: 1,   10: 1,   11: 0,   12: 0,   13: 0,   14: 0,   15: 0,   16: 0,   17: 0,   18: 0,   19: 0,   20: 0,   21: 0,   22: 0,   23: 0,   24: 0,   25: 0,   26: 0,   27: 0,   28: 0,   29: 0,   30: 0,   31: 0,   32: 0,   33: 0,   34: 0,   35: 0,   36: 0,   37: 0,   38: 0,   39: 0,   40: 2,   41: 0,   42: 0,   43: 0,   44: 0,   45: 0,   46: 0,   47: 1,   48: 0,   49: 0,   50: 0,   51: 1,   52: 2,   53: 0,   54: 0,   55: 0,   56: 0,   57: 0,   58: 1,   59: 0,   60: 0,   61: 0,   62: 0,   63: 0,   64: 0,   65: 0,   66: 0,   67: 0,   68: 0,   69: 0,   70: 0,   71: 0,   72: 0,   73: 0,   74: 0,   75: 0,   76: 0,   77: 0,   78: 0,   79: 1,   80: 0,   81: 0,   82: 0,   83: 0,   84: 0,   85: 0,   86: 0,   87: 0,   88: 0,   89: 0,   90: 0,   91: 0,   92: 1,   93: 0,   94: 0,   95: 0,   96: 0,   97: 0,   98: 0,   99: 0,   100: 0,   101: 1,   102: 0,   103: 0,   104: 0,   105: 2,   106: 1,   107: 0,   108: 0,   109: 2,   110: 0,   111: 0,   112: 0,   113: 0,   114: 0,   115: 0,   116: 0,   117: 0,   118: 1,   119: 0,   120: 2,   121: 0,   122: 0,   123: 0,   124: 0,   125: 0,   126: 0,   127: 0,   128: 0,   129: 0,   130: 1,   131: 0,   132: 0,   133: 0,   134: 0,   135: 1,   136: 0,   137: 0,   138: 0,   139: 0,   140: 0,   141: 1,   142: 0,   143: 0,   144: 0,   145: 0,   146: 0,   147: 0,   148: 1,   149: 0,   150: 0,   151: 0,   152: 0,   153: 0,   154: 0,   155: 0,   156: 0,   157: 1,   158: 0,   159: 0,   160: 0,   161: 1,   162: 1,   163: 1,   164: 0,   165: 0,   166: 0,   167: 0,   168: 0,   169: 0,   170: 1,   171: 1,   172: 0,   173: 0,   174: 0,   175: 0,   176: 1,   177: 0,   178: 1,   179: 0,   180: 0,   181: 2,   182: 1,   183: 0,   184: 1,   185: 2,   186: 3,   187: 0,   188: 0,   189: 0,   190: 2,   191: 0,   192: 0,   193: 0,   194: 0,   195: 0,   196: 0,   197: 0,   198: 0,   199: 0,   200: 0,   201: 2,   202: 0,   203: 1,   204: 2,   205: 0,   206: 0,   207: 0,   208: 0,   209: 0,   210: 4,   211: 0,   212: 0,   213: 0,   214: 0,   215: 0,   216: 1,   217: 0,   218: 2,   219: 2,   220: 0,   221: 0,   222: 0,   223: 0,   224: 0,   225: 4,   226: 0,   227: 0,   228: 0,   229: 0,   230: 0,   231: 0,   232: 0,   233: 2,   234: 0,   235: 0,   236: 0,   237: 0,   238: 1,   239: 3,   240: 0,   241: 3,   242: 1,   243: 0,   244: 1,   245: 0,   246: 2,   247: 0,   248: 0,   249: 0,   250: 0,   251: 0,   252: 0,   253: 0,   254: 0,   255: 0,   256: 0,   257: 0,   258: 0,   259: 0,   260: 2,   261: 0,   262: 0,   263: 3,   264: 0,   265: 1,   266: 0,   267: 0,   268: 0,   269: 1,   270: 2,   271: 0,   272: 0,   273: 0,   274: 0,   275: 3,   276: 0,   277: 0,   278: 0,   279: 0,   280: 0,   281: 2,   282: 0,   283: 2,   284: 0,   285: 0,   286: 0,   287: 0,   288: 2,   289: 0,   290: 0,   291: 0,   292: 1,   293: 0,   294: 0,   295: 0,   296: 2,   297: 0,   298: 0,   299: 2,   300: 0,   301: 0,   302: 0,   303: 0,   304: 0,   305: 2,   306: 0,   307: 0,   308: 0,   309: 1,   310: 1,   311: 1,   312: 1,   313: 0,   314: 1,   315: 0,   316: 1,   317: 0,   318: 0,   319: 0,   320: 0,   321: 1,   322: 0,   323: 0,   324: 4,   325: 1,   326: 0,   327: 0,   328: 0,   329: 0,   330: 0,   331: 4,   332: 0,   333: 0,   334: 0,   335: 2,   336: 2,   337: 5,   338: 1,   339: 0,   340: 1,   341: 1,   342: 1,   343: 2,   344: 3,   345: 1,   346: 0,   347: 5,   348: 0,   349: 1,   350: 2,   351: 2,   352: 7,   353: 1,   354: 2,   355: 1,   356: 1,   357: 0,   358: 1,   359: 1,   360: 2,   361: 0,   362: 0,   363: 0,   364: 0,   365: 5,   366: 0,   367: 1,   368: 0,   369: 1,   370: 0,   371: 0,   372: 7,   373: 1,   374: 1,   375: 3,   376: 0,   377: 0,   378: 1,   379: 2,   380: 0,   381: 3,   382: 0,   383: 0,   384: 3,   385: 0,   386: 1,   387: 1,   388: 1,   389: 0,   390: 1,   391: 0,   392: 2,   393: 1,   394: 10,   395: 0,   396: 0,   397: 0,   398: 4,   399: 1,   400: 6,   401: 4,   402: 4,   403: 1,   404: 1,   405: 0,   406: 0,   407: 2,   408: 0,   409: 2,   410: 0,   411: 1,   412: 1,   413: 0,   414: 0,   415: 8,   416: 1,   417: 0,   418: 4,   419: 4,   420: 2,   421: 1,   422: 5,   423: 1,   424: 2,   425: 1,   426: 0,   427: 1,   428: 1,   429: 11,   430: 4,   431: 3,   432: 2,   433: 1,   434: 0,   435: 0,   436: 5,   437: 0,   438: 0,   439: 0,   440: 5,   441: 1,   442: 0,   443: 1,   444: 8,   445: 0,   446: 0,   447: 0,   448: 0,   449: 2,   450: 2,   451: 2,   452: 6,   453: 1,   454: 0,   455: 5,   456: 7,   457: 1,   458: 0,   459: 0,   460: 0,   461: 1,   462: 2,   463: 1,   464: 5,   465: 1,   466: 1,   467: 1,   468: 5,   469: 2,   470: 7,   471: 4,   472: 3,   473: 0,   474: 3,   475: 4,   476: 1,   477: 0,   478: 7,   479: 6,   480: 1,   481: 1,   482: 1,   483: 4,   484: 1,   485: 5,   486: 7,   487: 1,   488: 3,   489: 3,   490: 5,   491: 3,   492: 1,   493: 1,   494: 5,   495: 1}})\n",
        "\n",
        "# Sentiment Data\n",
        "sentiment = pd.DataFrame([[0.81, \"Positive\"], [0.19, \"Negative\"]], columns=[\n",
        "    \"Sentiment\", \"Sentiment Type\"])\n",
        "\n",
        "# Sentiment Callouts\n",
        "most_positive_example = {\n",
        "    'text': '''I need those beluga reflective yeezys.''', 'user': 'Shayla.'}\n",
        "\n",
        "most_negative_example = {\n",
        "    'text': 'That beluga reflective isn’t the same.', 'user': 'BeXar'}\n",
        "\n",
        "callout_data = {\"most_positive\": most_positive_example,\n",
        "                \"most_negative\": most_negative_example}\n",
        "\n",
        "newLayout = html.Div(\n",
        "    id=\"root\",\n",
        "    children=[headerComponent, generate_content_component(finalPredictionDF, \"adidas Yeezy Boost 350 V2 Beluga Reflective\", sentiment, resampledRedditData, callout_data)])\n",
        "\n",
        "app.layout = newLayout\n",
        "\n",
        "\n",
        "\n",
        "app.run_server(mode=\"inline\", host = '127.0.0.1')\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Project",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
