# -*- coding: utf-8 -*-
"""Time series v2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1LBp2nboSgIZ7rmIyocb_6suFdLjGjeiP
"""


# socail media sentiment, result, name of shoe'

# Install the SQLAlchemy library if it is not installed
# !sudo apt-get install python3-dev libmysqlclient-dev > /dev/null
# !pip install mysqlclient > /dev/null
# !sudo pip3 install -U sql_magic > /dev/null
# !pip install psycopg2-binary > /dev/null

# ! pip install pmdarima
import plotly.express as px
from pmdarima.arima import auto_arima
import re
from datetime import date
from datetime import datetime
from bs4 import BeautifulSoup
import json
import requests
import matplotlib
import statsmodels.api as sm
import pandas as pd
import datetime

from sqlalchemy import create_engine
import time
import warnings
import itertools
import numpy as np
import matplotlib.pyplot as plt
warnings.filterwarnings("ignore")

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
# %config InlineBackend.figure_format = 'retina'


def get_pushshift_data(data_type,
                       q,
                       after,
                       size,
                       sort_type,
                       sort):
    url = f"https://api.pushshift.io/reddit/search/{data_type}/"
    parameter = {
        'data_type': data_type,
        'q': q,
        'after': after,
        'size': size,
        'sort_type': sort_type,
        'sort': sort,
        'aggs': 'subreddit'
    }
    request = requests.get(url, params=parameter)
    return request.json()


def dfpopulator(data, query, sku):
    dic = {'Key': [], 'sku': [], 'Body': [],
           'Time(GMT)': [], 'Subreddit': [], 'Subreddit_id': []}
    for item in data['data']:
        dic['Key'].append(query)
        dic['sku'].append(sku)
        dic['Body'].append(item['body'])
        dic['Time(GMT)'].append(item['created_utc'])
        dic['Subreddit'].append(item['subreddit'])
        dic['Subreddit_id'].append(item['subreddit_id'])

    df = pd.DataFrame(dic)
    return df


def redditSearch_Update(shoename, r_date, sku):
    sneakers = [shoename]
    data_type = 'comment'
    duration = '500d'
    size = 100
    sort_type = "created_utc"
    sort = 'desc'
    aggs = 'subreddit'
    poped = False
    column = ['Key', 'sku', 'Body', 'Time(GMT)', 'Subreddit', 'Subreddit_id']
    shoedic = {}

    shoedf = pd.DataFrame(columns=column)
    for sneaker in sneakers:
        utcdate = r_date
        epoch_date = utcdate.timestamp()
        data = get_pushshift_data(data_type,
                                  sneaker,
                                  utcdate,
                                  size,
                                  sort_type,
                                  sort)
        time.sleep(0.5)
        data = dfpopulator(data, sneaker, sku)
        shoedf = shoedf.append(data, ignore_index=True)
    shoedf['Time(GMT)'] = pd.to_datetime(shoedf['Time(GMT)'], unit='s')
    shoedf = shoedf.applymap(lambda x: x.encode(
        'ascii', 'ignore').decode('utf-8') if type(x) == str else x)
    conn_string = 'mysql://{user}:{password}@{host}:{port}/{db}?charset=utf8'.format(
        user='evergrand',
        password='13Mterz/PFg=',
        host='jsedocc7.scrc.nyu.edu',
        port=3306,
        encoding='utf-8',
        db='Evergrand'
    )

    engine_reddit = create_engine(conn_string)
    shoedf.to_sql(name='Reddit',  # name the table "inspections"
                  con=engine_reddit,  # use the connection to MySQL created earlier
                  if_exists='replace',  # if the table is already there, replace it
                  index=False,  # do not write the index column in the database
                  )


def twitterSearch_Update(q):
    counter = 0
    next_token = ''
    url = f"https://api.twitter.com/2/tweets/search/recent?query={q}&max_results=100&{next_token}tweet.fields=created_at"
    con = True
    column = ['created_at', 'id', 'text']
    dflist = []
    counter = 0
    while (con):
        # print(counter)
        counter += 1
        payload = {
        }
        headers = {
            'Authorization': 'Bearer AAAAAAAAAAAAAAAAAAAAAKYtWAEAAAAA2CtrQBw84WzcpTkYjnj%2BGKQ9bLg%3Dd5MPlt3uPPKp4dFVYbJEwPDFX5CuyB1d3CY4TyFZ3vr1fuVd5B',
            'Cookie': 'guest_id=v1%3A163753755452701433; guest_id_ads=v1%3A163753755452701433; guest_id_marketing=v1%3A163753755452701433; personalization_id="v1_ZpL8SwtB/R5mdhJ27jO/HQ=="'
        }

        response = requests.request("GET", url, headers=headers, data=payload)
        # print(response.text)

        result = response.json()
        if 'errors' in result:
            badresult = pd.DataFrame(columns=column)
            badresult['Name'] = ''
            badresult['Name'] = q
            badresult = badresult[['Name', 'created_at', 'id', 'text']]
            to_append = [q, None, None, None]
            a_series = pd.Series(to_append, index=badresult.columns)

            badresult = badresult.append(a_series, ignore_index=True)
            return badresult
        newdata = pd.DataFrame(result['data'])
        # print(newdata)
        dflist.append(newdata)
        # print(basedf)
        if ('next_token' in result['meta']):
            next_token = 'next_token='+result['meta']['next_token']+'&'
            url = f"https://api.twitter.com/2/tweets/search/recent?query={q}&max_results=100&{next_token}tweet.fields=created_at"
            time.sleep(1)

        else:
            con = False
    print(q)
    finalproduct = pd.concat(dflist)
    finalproduct['Name'] = ''
    finalproduct['Name'] = q
    finalproduct = finalproduct[['Name', 'created_at', 'id', 'text']]
    conn_string = 'mysql://{user}:{password}@{host}:{port}/{db}?charset=utf8'.format(
        user='evergrand',
        password='13Mterz/PFg=',
        host='jsedocc7.scrc.nyu.edu',
        port=3306,
        encoding='utf-8',
        db='Evergrand'
    )
    engine_tweeter = create_engine(conn_string)
#   %reload_ext sql_magic
#   %config SQL.conn_name = 'engine_tweeter'
    finalproduct = finalproduct.applymap(lambda x: x.encode(
        'ascii', 'ignore').decode('utf-8') if type(x) == str else x)
    print(finalproduct[1:5])
    finalproduct[1:5].to_sql(name='Tweets',  # name the table "inspections"
                             con=engine_tweeter,  # use the connection to MySQL created earlier
                             if_exists='replace',  # if the table is already there, replace it
                             index=False,  # do not write the index column in the database
                             )


"""#STEP 1: WEBSCRAPE RELEVANT INFORMATION

Gets different attributes of a shoe based on the url input
Attributes include name, release date, sku, and release price(MSRP)
"""


def getAttributeFromUrl(url):
    headers = {
        'sec-ch-ua': '"Chromium";v="94", "Google Chrome";v="94", ";Not A Brand";v="99"',
        'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/94.0.4606.71 Safari/537.36',    'sec-ch-ua-platform': '"Windows"',
        'sec-fetch-site': 'same-site',
        'sec-fetch-dest': 'empty',
        'accept-language': 'en-US,en;q=0.9',
    }
    r = requests.get(url, headers=headers)
    soup = BeautifulSoup(r.text)
    # test = soup.find_all('p',class_="chakra-text css-1h7lzbl")
    # print(test)
    release_date = soup.find_all('p', class_="chakra-text css-imcdqs")[3].text
    release_price = soup.find_all('p', class_="chakra-text css-imcdqs")[2].text

    # if (release_date.startswith('20')):
    #   datetime_obj = datetime.strptime(release_date,"%Y/%m/%d")
    # else:
    #   datetime_obj = datetime.strptime(release_date,"%m/%d/%Y")
    name = soup.find_all('p', class_="chakra-text css-exht5z")[0].text
    sku = re.findall(r'sku":"(.*)","c', r.text)[0]
    placeholder = 1
    valuelist = [name, placeholder, sku, release_price]
    return valuelist


"""Gets average daily sales data from StockX"""


def getSalesFromInput(shoeurl, sku, today):
    url = "https://stockx.com/p/e"

    payload = json.dumps({
        "operationName": "FetchSalesGraph",
        "variables": {
            "isVariant": False,
            "productId": sku,
            "startDate": "all",
            "endDate": today,
            "intervals": 100,
            "currencyCode": "USD"
        },
        "query": "query FetchSalesGraph($productId: String!, $currencyCode: CurrencyCode, $intervals: Int, $startDate: String, $endDate: String, $isVariant: Boolean! = false) {\n  variant(id: $productId) @include(if: $isVariant) {\n    salesChart(\n      currencyCode: $currencyCode\n      intervals: $intervals\n      startDate: $startDate\n      endDate: $endDate\n    ) {\n      ...SalesGraph\n      __typename\n    }\n    __typename\n  }\n  product(id: $productId) @skip(if: $isVariant) {\n    id\n    title\n    productCategory\n    salesChart(\n      currencyCode: $currencyCode\n      intervals: $intervals\n      startDate: $startDate\n      endDate: $endDate\n    ) {\n      ...SalesGraph\n      __typename\n    }\n    __typename\n  }\n}\n\nfragment SalesGraph on SalesChart {\n  series {\n    xValue\n    yValue\n    __typename\n  }\n  __typename\n}\n"
    })
    headers = {
        'authority': 'stockx.com',
        'sec-ch-ua': '" Not A;Brand";v="99", "Chromium";v="96", "Google Chrome";v="96"',
        'apollographql-client-name': 'Iron',
        'accept-language': 'en-US',
        'sec-ch-ua-mobile': '?0',
        'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36',
        'content-type': 'application/json',
        'accept': '/',
        'apollographql-client-version': '2021.12.16.1',
        'sec-ch-ua-platform': '"Windows"',
        'origin': 'https://stockx.com',
        'sec-fetch-site': 'same-origin',
        'sec-fetch-mode': 'cors',
        'sec-fetch-dest': 'empty',
        'referer': shoeurl,
        'cookie': '_ga=GA1.2.1853109428.1596137523; tracker_device=7f932b85-ff29-4b1f-a978-ce2e2afb45f1; _fbp=fb.1.1596137526024.1799719658; _px_f394gi7Fvmc43dfg_user_id=NWFhMzBiNjAtZDI5Yi0xMWVhLWJkMjUtZDVmMDk0ZDViODky; rskxRunCookie=0; rCookie=toixn79vpovnwmk8gutmmkd972mb6; QuantumMetricUserID=478d7a5bc264c78c0a6e7c5ff5d6cbde; mfaLogin=err; stockx_experiments_id=web-fc31ed57-2684-4bcc-a299-afd5de7e116d; _rdt_uuid=1621275189532.77d09608-2d35-461b-9e0f-d95973065349; _pk_id.421.1a3e=fa6ab7db943ac79b.1608758091.61.1622714543.1622714535.; __pdst=d6567c5c9b7843fc8ba6464d49be91d6; ajs_user_id=23303495-a855-11e7-afa5-128878ebb8b6; _ts_yjad=1629999961938; __ssid=6987566f28885ba3f26e3181d727ad9; _scid=1221c84a-c00d-4175-8c66-414b5227b647; _pxvid=d5c0f182-1723-11ec-aaf1-645079574d4f; _ga=GA1.2.1853109428.1596137523; _pin_unauth=dWlkPVpEZzJNek00WkRndE1XRTNNaTAwWTJJM0xUbGxPVEl0TUdJNE5EZ3hPREZsTXpNMA; __pxvid=3665192f-3795-11ec-a1ec-0242ac120003; _gcl_au=1.1.1441916943.1636602572; ajs_group_id=ab_3ds_messaging_eu_web.false%2Cab_aia_pricing_visibility_web.novariant%2Cab_chk_germany_returns_cta_web.true%2Cab_chk_order_status_reskin_web.false%2Cab_chk_place_order_verbage_web.false%2Cab_chk_recently_viewed_order_confirmation_web.false%2Cab_chk_remove_affirm_bid_entry_web.true%2Cab_chk_remove_fees_bid_entry_web.true%2Cab_citcon_psp_web.true%2Cab_desktop_home_hero_section_web.control%2Cab_home_contentstack_modules_web.variant%2Cab_low_inventory_badge_pdp_web.control%2Cab_pirate_payment_reorder_v2_web.variant_3%2Cab_pirate_recently_viewed_browse_web.false%2Cab_product_page_refactor_web.true%2Cab_recently_viewed_pdp_web.variant_1%2Cab_test_korean_language_web.true%2Cab_web_aa_1103.true; ajs_user_id=23303495-a855-11e7-afa5-128878ebb8b6; ajs_anonymous_id=4629f2c4-16f5-40ba-81c1-423ba2ef012b; token=; ajs_group_id=ab_3ds_messaging_eu_web.false%2Cab_aia_pricing_visibility_web.novariant%2Cab_chk_germany_returns_cta_web.true%2Cab_chk_order_status_reskin_web.false%2Cab_chk_place_order_verbage_web.false%2Cab_chk_remove_affirm_bid_entry_web.true%2Cab_chk_remove_fees_bid_entry_web.true%2Cab_citcon_psp_web.true%2Cab_desktop_home_hero_section_web.control%2Cab_home_contentstack_modules_web.variant%2Cab_home_dynamic_content_targeting_web.variant%2Cab_low_inventory_badge_pdp_web.control%2Cab_pirate_recently_viewed_browse_web.false%2Cab_product_page_refactor_web.true%2Cab_recently_viewed_pdp_web.variant_1%2Cab_test_korean_language_web.true%2Cab_web_aa_1103.true; ajs_anonymous_id=4629f2c4-16f5-40ba-81c1-423ba2ef012b; language_code=en; stockx_market_country=US; _gid=GA1.2.1580341715.1639858083; pxcts=34d1db51-603e-11ec-b2c9-379611497244; hide_my_vices=false; riskified_recover_updated_verbiage=true; ops_banner_id=blt055adcbc7c9ad752; IR_gbd=stockx.com; stockx_preferred_market_activity=sales; stockx_default_sneakers_size=All; stockx_default_streetwear_size=All; _clck=sj015u|1|exe|0; QuantumMetricSessionID=26abe8546fdd386fb58cbb0ec2466002; stockx_homepage=sneakers; stockx_session=%221710e3b6-416f-4848-bb6b-ffc319d1cc10%22; stockx_product_visits=5; IR_9060=1639873437892%7C0%7C1639873437892%7C%7C; IR_PI=59c8d844-d29b-11ea-a852-42010a246e2b%7C1639959837892; forterToken=ecbab609917d450281f829178b7e7def_1639873437228__UDF43_13ck; lastRskxRun=1639873440198; _px3=09501e9b6bf87059145143c8edfa674d1045e21fbf31ee08c7affe41e863d8bd:WGN7FyBy6rerAupD/A7AlTvSTGeHPH4KvI11cOn1VhFz6ygQY3HHamxbp6Uyt1bdyq21I8E6JMIHn42Jss+eTg==:1000:aIVG2m+sR6rb1di3NBqFjci4JZrtErPPhG28tvjv7xfexK+WZmAE2yLm9kRAMtsJM1lDl8Yy253gXx9Z9BclHw0q1UBA4fpzfI+j8F+Axd2LRXdM32fTKECUQd3qUJzJFYnN9q0SFg5LfbV0QltM+JgUaa8WKcG8AoUsZPG0CTQL91lSdXCkuXPovctSwW/FAf6H/rJwLukypQJOWfMiNWhcHJcTZkm7ZRQ6Ngn+PoM=; _clsk=wdnm0l|1639873780294|4|0|e.clarity.ms/collect; _gat=1; _uetsid=37a92700603e11ecbed3ebddd1d15633; _uetvid=c97149c0c3c111eba8f38109381a970d; _px_7125205957_cs=eyJpZCI6IjdiMDc1ZGQwLTYwNjAtMTFlYy05MzE3LTk5ZmE4Yzg5OGYyYSIsInN0b3JhZ2UiOnt9LCJleHBpcmF0aW9uIjoxNjM5ODc1NTg3OTk4fQ==; _dd_s=rum=0&expire=1639874688855'
    }

    response = requests.request("POST", url, headers=headers, data=payload)

    return response


def exo_convert(exo):
    nobs = len(exo.index)
    k = len(exo.columns)
    print(nobs, " and k: ", k)

    exog = np.empty([nobs, k])
    for i in range(nobs):
        k = 0
        for column in exo.columns:
            exog[i, k] = exo[column].iloc[i]
            k += 1

        i += 1
    return exog


def getSentiment(text):
    endpoint = "https://api.us-east.natural-language-understanding.watson.cloud.ibm.com/instances/041cfe51-46b7-404a-baf4-88c9375ddb0c/v1/analyze"

    username = "apikey"
    password = "VqjPYYm8k1aHaYjkQpW1SimhfBmKLO6kjgSHAV4OZIo-"

    parameters = {
        'features': 'emotion,sentiment',
        'version': '2021-08-01',
        'text': text,
        # 'language' : 'en',
        # 'encoding' : 'utf-8'
    }

    resp = requests.get(endpoint, params=parameters, auth=(username, password))
    # if (resp.status_code == 400):
    #     print(resp.text)
    #     print(text)
    return resp


"""get sales data and transform them into more readable forms (changing column names to understandable values

"""


def main(shoeurl):
    # assuming url has been received from user
    # get sku from url
    attributes = getAttributeFromUrl(shoeurl)
    sku = attributes[2]
    today = date.today().strftime("%Y-%m-%d")
    salesdic = getSalesFromInput(shoeurl, sku, today)
    sales = (salesdic.json())['data']['product']['salesChart']['series']
    dcol = ['__typename', 'xValue', 'yValue']

    allsales = pd.DataFrame(sales)
    print(attributes)
    allsales['Date'] = pd.to_datetime(allsales['xValue']).dt.date
    allsales['Date'] = pd.to_datetime(allsales['Date'])
    allsales['Price'] = allsales['yValue']
    allsales.drop(dcol, axis=1, inplace=True)
    allsales.dtypes
    px.line(allsales, x='Date', y='Price')
    # plt.show()

    """manually enter day zero sales data

  day zero sales means the shoe sales in its retail price

  the price of shoes tend to be much greater than the retail price, so here, I took the retail price, and entered its date as (earlest sales record date - 1 day) to represent the day 0 price
  """
    datefirst = allsales['Date'][0]-pd.Timedelta(days=1)
    name = attributes[0]
    twitterSearch_Update(name)
    redditSearch_Update(name, datefirst, sku)

    dayone = int(attributes[3][1:4])
    start = {'Date': datefirst, 'Price': dayone}
    allsales = allsales.append(start, ignore_index=True, sort=True)

    allsales.sort_values('Date', inplace=True)
    print(allsales)

    PCT = allsales
    PCT['Daily_Change'] = allsales['Price'].pct_change()
    duration = PCT['Date'].iat[-1]-PCT['Date'].iat[0]
    if (duration > pd.Timedelta(days=30)):
        date_offset = 14
    else:
        date_offset = 7

    # PCT.drop(['Price'],axis = 1,inplace = True)
    PCT = PCT.fillna(0)
    PCT

    px.line(PCT, y='Daily_Change')

    """eliminate the duplicate dates that sometimes appear in stockx's website. 
    """

    PCT = PCT.drop_duplicates(subset=['Date'], keep='last')
    PCT
    # PCT['Date']=PCT['Date'].asfreq('d')
    # PCT = PCT.set_index('Date')
    # PCT[PCT.index.duplicated()]

    PCT = PCT.set_index('Date').asfreq('d')

    PCT

    PCT.drop('Price', axis=1, inplace=True)

    pd.set_option("display.max_rows", 10)

    PCT = PCT.fillna(0)
    PCT.index.isnull().sum()
    PCT

    print(PCT.index.freq)

    """Note to self: remmeber to implement automatic find best

  #Get Reddit posts and integrade into a dataframe of exogenous variables
  """

    conn_string = 'mysql://{user}:{password}@{host}:{port}/{db}?charset=utf8'.format(
        user='evergrand',
        password='13Mterz/PFg=',
        host='jsedocc7.scrc.nyu.edu',
        port=3306,
        encoding='utf-8',
        db='Evergrand'
    )
    engine_visual = create_engine(conn_string)

    # Commented out IPython magic to ensure Python compatibility.
    # %reload_ext sql_magic
    # %config SQL.conn_name = 'engine_visual'

    # Commented out IPython magic to ensure Python compatibility.
    # %%read_sql
    # show tables

    query = f'SELECT * FROM Reddit WHERE sku LIKE "{sku}"'
    reddit_data = pd.read_sql(query, engine_visual)
    reddit_data

    query2 = f'SELECT * FROM Tweets WHERE Name LIKE "{name}"'
    twt_data = pd.read_sql(query2, engine_visual)
    twt_data
    # query2 = f'SELECT * FROM Tweets WHERE Name LIKE "{name}"'
    # twt_data = pd.read_sql(query2,engine_visual)
    # twt_data

    import requests
    import json

    twt_emotion = {
        "sadness": [],
        "joy": [],
        "fear": [],
        "disgust": [],
        "anger": []
    }
    twtbodies = twt_data['text']
    for body in twtbodies:
        sentiment = getSentiment(body).json()
        # print(sentiment)
        # if sentiment['code'] == '400':
        #   continue
        for emotion in twt_emotion:
            try:
                twt_emotion[emotion].append(
                    sentiment['emotion']['document']['emotion'][emotion])
            except Exception as e:
                continue

    twtbodies

    reddit_emotion = {
        "sadness": [],
        "joy": [],
        "fear": [],
        "disgust": [],
        "anger": []
    }
    bodies = reddit_data['Body']
    for body in bodies:
        sentiment = getSentiment(body).json()

        for emotion in reddit_emotion:
            reddit_emotion[emotion].append(
                sentiment['emotion']['document']['emotion'][emotion])

    bodies

    twt_emotion

    reddit_emotion

    for emotion in reddit_emotion:
        reddit_data[emotion] = reddit_emotion[emotion]

    for emotion in twt_emotion:
        twt_data[emotion] = twt_emotion[emotion]

    twt_data
#   print(twt_data,"\n\n\nand reddit_data",reddit_data)
    t_twt = twt_data

    t_reddit = reddit_data

#   reddit_data['Time(GMT)'] = pd.to_datetime(reddit_data['Time(GMT)'])
    t_reddit['Date'] = pd.to_datetime(reddit_data['Time(GMT)'].dt.date)
    t_reddit = t_reddit.set_index('Date')

    t_twt['Date'] = pd.to_datetime(t_twt['created_at'])

    t_twt = t_twt.set_index('Date')

    t_twt = t_twt.resample('d').mean()
    t_twt

    t_reddit

    t_reddit.dtypes

    t_reddit

    t_reddit.dtypes

    drop_columns = ['sku', 'Body', 'Subreddit',
                    'Subreddit_id', 'Key', 'Time(GMT)']
    t_reddit.drop(drop_columns, axis=1, inplace=True)

    time_index = PCT.index.to_series(name='exo')
    time_index

    time_index.asfreq('d')
    time_index

    t_reddit.sort_index(inplace=True)
    exo = t_reddit.merge(time_index, how='right', on='Date')
    # exo2 = pd.concat([time_index,t_twt], axis = 0,join = 'outer',ignore_index = False)
    twt_index = pd.to_datetime(
        t_twt.index.to_series(name='twt').dt.date).asfreq('d')
    twt_index

    t_twt['New Date'] = twt_index
    t_twt = t_twt.reset_index()
    t_twt['Date'] = t_twt['New Date']
    t_twt = t_twt.set_index('Date')
    t_twt
    exo2 = t_twt.merge(time_index, how='right', on='Date')
    exo.drop(['exo'], axis=1, inplace=True)
    exo2.drop(['exo', 'New Date'], axis=1, inplace=True)

    # for column in range(len(exo.columns)):
    #   for i in range(len(exo.index)):
    #     if (pd.isna(exo.iloc[i,column]) and !pd.isna(exo2.iloc[i,column])):
    #       exo.iat[i,column] == ((exo.iat[i,column]+exo2.iat[i,column])/2)
    #     elif pd.isna(exo2.iat[i,column]):
    #       exo.iat[i,column] = exo2.iat[i,column]
    #     else:
    #       exo.iat[i,column] = 0
    # exo.iloc[1,3] = 10
    exo = exo.combine_first(exo2)
    media_sentiment = exo
    pd.options.display.max_rows = 999
    exo.fillna(method='bfill', inplace=True)
    exo.fillna(method='ffill', inplace=True)
    exo.replace(to_replace=0, method='ffill', inplace=True)
    exo.replace(to_replace=0, method='bfill', inplace=True)

    print("\\\\\\\ntest exo", exo)

    """Create off set in emotion values by 1 week"""

    inx = exo.index
    inx = inx.shift(date_offset, freq='D')
    exo.index = inx
    exo.index.freq

    """Create train test split

  Note that PCT train set starts on the same day that exo(which has been shifted by 7 days)
  """

    end_date = PCT.index[len(PCT.index)-1]
    end_date
    pred_end = end_date + pd.Timedelta(days=date_offset)
    pred_splittime = end_date + pd.Timedelta(days=date_offset)
    pred_exo = exo.loc[(exo.index > end_date) & (exo.index <= pred_end)]
    exo_full = exo.loc[(exo.index >= exo.index[0]) & (exo.index <= end_date)]
    PCT_full = PCT_train = PCT.loc[(
        PCT.index >= exo.index[0]) & (PCT.index <= end_date)]
    pred_exo = exo_convert(pred_exo)
    exo_full = exo_convert(exo_full)

    print("\\\\\\\ntest exo_full ", exo_full)
    print("\\\\\\\ntest pred_exo ", pred_exo)
    print("\\\\\\\ntest PCT_full ", PCT_full)

    split_time = end_date - pd.Timedelta(days=date_offset)
    exo_train = exo.loc[(exo.index >= exo.index[0])
                        & (exo.index <= split_time)]
    exo_test = exo.loc[(exo.index > split_time) & (exo.index <= end_date)]
    print("\\\\\\\ntest exo", exo_test)
    PCT_train = PCT.loc[(PCT.index >= exo.index[0])
                        & (PCT.index <= split_time)]
    PCT_test = PCT.loc[(PCT.index > split_time) & (PCT.index <= end_date)]

    PCT_print = PCT.loc[(PCT.index >= exo.index[0]) & (PCT.index <= end_date)]
    PCT_test

    pd.set_option("display.max_rows", 114)

    exo_train

    exo_train = exo_convert(exo_train)
    exo_test = exo_convert(exo_test)
    from pylab import rcParams
    rcParams['figure.figsize'] = 18, 8
#   decomposition = sm.tsa.seasonal_decompose(PCT_train, model='additive')
#   fig = decomposition.plot()
    # plt.show()

#   paramaters = auto_arima(PCT_train,max_p = 7,max_d = 2,trace = True)
#   paramaters

    d = q = range(0, 2)
    p = range(0, 7)
    pdq = list(itertools.product(p, d, q))
    seasonal_pdq = [(x[0], x[1], x[2], 53)
                    for x in list(itertools.product(p, d, q))]

    # for param in pdq:
    #     for param_seasonal in seasonal_pdq:
    #         try:
    #             mod = sm.tsa.statespace.SARIMAX(PCT_train,
    #                                             order=param,
    #                                             seasonal_order=param_seasonal,
    #                                             enforce_stationarity=False,
    #                                             enforce_invertibility=False)
    #             results = mod.fit()
    #             current = results.alc
    #             print('ARIMA{}x{}12 - AIC:{}'.format(param, param_seasonal, results.aic))
    #         except:
    #             continue

    print(PCT_full.shape)

    print(exo_full.shape)

    mod = sm.tsa.statespace.SARIMAX(PCT_full,
                                    exog=exo_full,
                                    order=(7, 1, 0),
                                    seasonal_order=(0, 0, 0, 53),
                                    enforce_stationarity=False,
                                    enforce_invertibility=False)
    results = mod.fit()
#   print(results.summary().tables[1])

#   results.plot_diagnostics(figsize=(16, 8))
    # plt.show()

    """#In sample testing"""

    end_date = PCT.index[len(PCT.index)-1]

    end_date

    pred_offset = pd.Timedelta(days=date_offset)
    pred_start = end_date - pred_offset
    pred_start
    print(pred_start)

    pred = results.get_prediction(start=pred_start, dynamic=False)

    print(pred.predicted_mean)

    pred = results.get_prediction(start=pred_start, dynamic=False)
    pred_con = pred.conf_int()

    ax = PCT_train.plot(label='observed')

#   pred.predicted_mean.plot(ax = ax,label = 'One step ahead',alpha = 0.7,figsize = (14,7))

#   ax.fill_between(pred_con.index,
#                   pred_con.iloc[:, 0],
#                   pred_con.iloc[:, 1], color='k', alpha=.2)

    ax.set_xlabel('Date')
    ax.set_ylabel('Percent change in price')
    plt.legend()
#   plt.show()

    type(pred.predicted_mean)

    price_forecasted = pred.predicted_mean
    price_truth = PCT_train[pred_start:]
    mse = ((price_forecasted - price_truth['Daily_Change']) ** 2).mean()
    print(f'The Mean Squared Error of our forecasts is {mse}')

    print('The Root Mean Squared Error of our forecasts is {}'.format(
        round(np.sqrt(mse), 10)))

    """#Out of sample testing"""

#   end_date2 = PCT.index[-1]
    end_date

    pred_offset = pd.Timedelta(days=date_offset)
    pred_start = end_date - pred_offset
    pred_start
    pred_endDate = end_date + pred_offset
#   print(end_date,"\n\n\n\n\n\n\nhelllo \n\n\n\n\n\nand exotest: ",exo_test)
    start_time = end_date + pd.Timedelta(days=1)
#   forecast = results.get_forecast(step = 1,exog = pred_exo,dynamic = False)
#   print(forecast)

    print(start_time, "\n\n\n\n\n\n\nhelllo \n\n\n\n\n\nand pred_enddate and PCT FULL: ",
          pred_endDate, "and PCT FULL: ", PCT_full)
    exo3 = exo_test+exo_test
#   print(end_date,"\n\n\n\n\n\n\nhelllo \n\n\n\n\n\nand exo3: ",exo3)

    pred2 = results.predict(start=start_time, end=pred_endDate, exog=pred_exo)

    # b = (exo_test.iloc[0]).reshape(1,5)
    # pred2 = results.get_forecast(step = 1,exog = exo_test[1],dynamic = False)
    pred_con = pred.conf_int()

    ax2 = PCT_train.plot(label='observed')

#   pred2.plot(ax = ax2,label = 'One step ahead',alpha = 0.7,figsize = (14,7))

#   ax2.fill_between(pred_con.index,
#                   pred_con.iloc[:, 0],
#                   pred_con.iloc[:, 1], color='k', alpha=.2)

    ax2.set_xlabel('Date')
    ax2.set_ylabel('Percent change in price')
#   plt.legend()
    print("\n\n\n\n\n\n\nhelllo  \n\n\n\n\n\nand the pred2: ", pred2)

    last_price = allsales["Price"].iat[-1]

    return (pred2, twt_data, reddit_data, name, last_price)


#   plt.show()
if __name__ == '__main__':
    main('https://stockx.com/adidas-yeezy-boost-350-v2-beluga-reflective')
